2023-03-13

the great start. debated long whether to use equinox or flax. lenart
recommends/uses flax. but the basic example seemed really convoluted, as
did the syntax for creating NNs. I think I will use equinox, it seems more
minimalistic while also requiring less boilerplate code (it seems), and the
complete compatibility with jax functions seems nice.

probably doesn't matter much either way, both libraries are quite small and
most of the boilerplate is a decision of the developer (how to handle
state and stuff).

am coding a small supervised learning example with a toy dataset with
equinox.

it works! am trying out lots of different optimizers from optax and
making 3d plots of the test error depending on training iteration and
learning rate. just to get a feel for things :)

adam, adabelief and yogi seem very nice for lr around 10^(-2.2) to
10^(1.4). adam probably the most robust so not much need to play around.

also did some experiments with continously sampled training data, still
supervised though. there is an interesting thing happening when we change
batch sizes. smaller batch sizes are more efficient per data point, but
incur more overhead in the training loop. would be interesting to do an
experiment where we plot the test error depending on wall training time to
find the practical optimum.

or, practically for now, find some settings by hand that work, and then in
the end throw all the hyperparams into some bayesian optimisation thing
that tries to achieve minimum test error in some given (wall) time frame.

good first day. learned a lot, gained confidence, did not get bored from
9:30 to 17:45.

2023-03-14

pivoted from optimizer tuning to batch size tuning and making nice plots.
it seems that w.r.t. wall time, batch sizes of 256-1024 are in the sweet
spot, with larger batch sizes being worse initially but better if trained
longer.

also did some nice 3d visualisations of a network training.

Noticed that I have a NN of the form y = x.T @ x, where x is the output of
the last layer. If instead we append a general layer to the NN somehow it
stops working, and somehow fits an almost constant function. TODO find out
why.

spent most of the afternoon debugging that, did not get far. instead only
found confusing stuff.

when the NN returns the squared norm of last layer activation (the
situation where everything works quite well) the gradient norm keeps
increasing during training, even though the loss decreases to 0.1 or 0.001
which is low enough according to the plot. the individual gradients look to
be mostly less than 0.2 in magnitude, with the largest ones growing a bit
above 1.

with the other 'standard' NN configuration (linear last layer with scalar
output), the test loss quickly stabilises quite exactly to about 5, with
the monte-carlo training loss between 6 and 11. here, the gradient norm
does decrease, but only very slowly, and the fit is poor.

made some plots to show lenart tomorrow.







2023-02-15

meeting w/ lenart and bhavya

supervised learning to approximate mpc? -> practical solution.

ct: value iteration? soft actor critic?


lenart's research.
learn probabilistic model of dynamics.
dt: q function. ct: problem bc u no influence on 'next' value. BUT value
derivative. this is where hjb comes from. value function does not depend on
u.


plan: formulate possible paths to take:
- hjb solution with known dynamics f
    this they kind of discourage bc. we need to "learn" the model f, and if
    we know f already then approximated MPC is the practical answer.
- approximated mpc
    practical approach to the problem at work, also interesting things to
    do with uncertainty sampling/active learning, stuff like that.
- value function estimate directly from sim data (red on whiteboard)
    this is basically RL.
- stuff on left of whiteboard

dt/ct which route to take? they say dt is probably easier. ct seems kind of
cleaner though. dunno

also TODO:
- read jax tutorial lenart sends
- read papers lenart sends
- get an overview of classic RL algorithms
- read DPOC notes, specifically the leap from discrete to continous time.

next week 9-10.



2023-03-21

reading some more papers. read three yesterday, was too much, head was
full.

did some small experiment, with solving the simplest possible PDE
(basically grad_x f(x) = 2x, with solution x.T x). it seems to scale
not-very-well with the dimension of the problem... but also, hyperparams
are quite crucial. it seems like small networks already do quite well, and
the common wisdom of large nets being simpler to train does not seem to
apply here. tried some decreasing learning rate stuff, worked really
nicely.

I am thinking that what lenart and bhavya suggest, i.e. going the RL-route
to construct a control policy from sim data, is probably a more fruitful
project.

as for pure practicality, they suggest (at work) that MPC -> NN
approximation is probably the way to go, which I also think is right.

possible roadmaps:

1.  do a 'regular' RL-type project, going from sim data to control policy,
    certainly also towards the computationally easier side of RL problems.

    fundamental question: how do we go from sim data to approximately optimal
    control policy, without writing down the model (again)?  maybe focus on
    data efficiency, safety/verifiability, something like that?

1a. do a RL-type project, but focus on making *some* aspect of it continuous
    time? the stuff they talked about basically.

2.  go all in on the pde-type approach, do hyperparam optimisation, find
    scaling laws etc. more exploratory than immediately practical.

    fundamental question: how can 'deep galerkin methods' be made practical
    in an optimal control context? how do we find a NN description of
    optimal value function or similar, WITH availability of a continuous-time
    model?

3.  do the ultra-practical option of MPC -> approximate control inputs with
    NN. surely the easiest in terms of extra engineering efforts, but also
    not many interesting results probably? do that at work sometime :)

    explore further options: theoretically sound, investigate the error, do
    robust mpc to start with. scalability? error bound with hoeffding eq.




2023-03-22

{{{
talk with bhavya and lenart.

bhavya concerns about possibility to deliver? decide on something to
investigate? -> do not rush, even 'non-results' are results, they are
basically open to everything, but want me to know what to expect.


hjb, model based approach theoretically quite clear. certainly interesting
and feasible, even if not very practical immediately.


also very exploratory ideas, which they brainstormed about quite long.

cont time rl ideas... all with parameterized policy: they see some
difficulties. lots of questions about how exactly to interface continuous
<-> discrete time, bc. measurements are only available in discrete time
anyway... something like discrete reward measurements -> estimate
continuous time optimal V, run controller in ct (faster?) lots of
interesting questions, lots of complications.

Q function in terms of x, u, and Δt? include Δt in the action space?
only change u a couple of times over time horizon? optimize over timing of
u changes? advantage of changing controller in reward somehow?
variable-rate control? very crazy... they throw around lots of ideas and I
understand probably half of it. cool idea, lots of missing pieces.

maybe ct rl is just not worth it \o/

do the decision whenever I want to and like to. sign up on mystudies once
the roadmap is clear, in 3-4 weeks they would start to "ask again" whether
I signed up.
}}}



did some more experiments. basically only hyperparam tuning on quadratic
pde example. learning rate schedules, network sizes, 'loop unrolling' with
multiple gradient steps in jit region.

lr schedule has big impact and is probably worth putting in some bayesian
hyperparam optimisation thing. network sizes maybe too, but mainly
influence computation time (still interesting if hyperparam opti is w.r.t.
fixed wall time.) unrolling probably not worth it, small speedup only but
complications with training indices and lr schedules.

todo, maybe also at work sometime: try jax for trajectory optimisation
(and everything else?)
https://github.com/google/trajax
https://github.com/nikihowe/myriad


2023-03-27

small experiments with training hyperparams. still just grad f(x) = 2x PDE.
jax.lax.fori_loop makes a multiple-gradient-step much nicer, fast jit, now
it is actually worth it.


2023-03-28

read some more papers, they are all in the appropriate semanticscholar dir.

some thoughts on the different options for the projects.

1. (classic RL)
   lots of examples & research
   highly practical as well
   good area to get into hypothetically

1a. (RL but focus on continuous time)
   some theoretical challenges, question practicality

2. pure PDE solver type approach
   very non-practical probably
   some possible upsides to other methods?
    - better handle on nonconvexity than predictive control
    - also nonconvex constraints - but constraints generally a bit harder
      again...

3. approximate MPC
   probably boring, very practical, basically done by
   Hertneck/Köhler/Trimpe/Allgöwer 2018. Neural&stochastic pendant of
   explicit MPC.



tbh, probably it will still be 2.

stuff to do:

- literature review & define goals for experiments
  possible options:
  - parameterisations of V
  - play around with different loss formulations
  - random vs adversarial sampling - uncertainty sampling? -> easier problem probably they say
  - go a bit into theory about nondifferentiability/viscosity? --> probably hard/unpractical they say
  - find connections to lygeros 'find largest V such that V <= TV' idea from the ADP paper?
  - principled, end-to-end hyperparam search?
    (optimising for MC control cost w/ fixed training time)
  - do something with state constraints? growing outer or shrinking inner penalty function? explicit lagrangian multiplier by another NN?
    safety verification type stuff...
  - investigate connection to various RL algorithms?
- experiment
- write


basically two broad directions:
- more theoretical (infinite-dim optimization with NN, viscosity solutions, etc)
- more practical (make it work with ML tricks)



we have 3 months basically. possible roadmap? to be adjusted continuously :)

months from start (30d each)
|------------------------------|------------------------------|------------------------------|
weeks (7d)
|------|------|------|------|------|------|------|------|------|------|------|------|------|------
^             ^             ^                                  ^             ^
|             |             |                                  |             start ONLY writing
|             |             |                                  finalize experiments (collect data, make plots, ...)
|             |             start experiments (adapt if too long/elaborate :)
|             start defining experiments?
literature review



2023-03-29

continue:
literature review about interesting stuff
present: state of the art and direction to investigate more. slides/whiteboard/whatever

try https://www.notion.so/


so, literature review time.

reading list for next time:

https://arxiv.org/pdf/2302.13122.pdf,
https://arxiv.org/pdf/2302.09878.pdf, and
https://arxiv.org/pdf/2002.08625.pdf but only skim, very math heavy

https://arxiv.org/pdf/2102.11802.pdf some neural pde stuff overview
dpoc script - hjb eq again.

one or two simpler ones from the direct hjb solving folder.



2023-04-04

am quite optimistic atm about an approach i thought about for work once.
basically, instead of directly taking the hjb eq., we start from the
pontryagin principle, but solve it backwards. so instead of the usual
problem (search for λ(0) leading to the desired x(T)) we have the reverse
-- start at some x(T) and integrate the adjoint system backwards, so we
arrive at lots of (state, value) pairs.

the good thing is we don't have to search for some exact initial state by
guessing the correct final state -- we want ALL the initial states. so
there is bound to be some sort of MC+uncertainty sampling method that does
quite well. the backwards adjoint system is probably unstable though so
have to watch out for that. probably we end up with yet another MC pde
solver with adaptive resampling. which is nice after all :)

found out that this is probably a method of characteristics:
https://en.wikipedia.org/wiki/Method_of_characteristics

we are bound to run into instability problems with this method. the
(optimal) forward system is stable, so the backward one will be unstable in
all dimensions, so the fastest dynamics will mess us up first.

can be addressed by various methods.

- iterative backward sim - function fitting thing, akin to backward DP
- completely adaptive starting and stopping of characteristics with some
  form of uncertainty sampling
- dont use pontryagin but nonlinear mpc
  together with many x0 sampled over state space, maybe with SGLD to only
  travel small amounts for good initialization, and a method to fit 'the
  lowest plausible value function' (see lygeros ADP paper - largest
  underestimator), we might still get some form of global optimality

now, i need to make slides and for that i need to tell some kind of story.
i would love to start with the control/rl comparison and then show how i am
exploring a new-ish niche in between the two -- solving control people's
problems using ML tools.


2023-04-23

https://www.youtube.com/playlist?list=PLHyI3Fbmv0ScvcPAT9IK6YPEZGnj-6Sqk


2023-04-24

setup eth workstation. did everything with virtualenv, very nice, only had
to fiddle a bit to get matplotlib to use a working GUI backend.

implemented resampling step properly (well, without function approximation,
so it still does nothing). but the business logic around that is nice and
works. atm I have a condition where we resample the sample that have a
mahalanobis distance >= some threshold from the initial state distribution.

maybe trying to resample as few trajectories as possible is not smart.
ranted we get the benefit of longer periods of 'exact' backwards ODE
integration, BUT the distribution of states may become undesirable,
practically it often becomes a distribution with no samples in large
regions and many samples in some regions. this might be fixed by one of two
things:

 - complete resampling after only a relatively short time (i.e. such that
   the distribution of states does not qualitatively change a lot - maybe
   also triggered by some condition relating distribution mismatch between
   sampling distribution and actual distribution of trajectories, like data
   likelihood or KL divergence). then we always stay at the sampling
   distribution and can nicely control which states are visited, BUT incur
   extra error due to the frequent alternation between sample-based and
   function-based representations.

 - some smarter thing where we first simulate a small-ish amount of
   pontryagin optimal trajectories backwards, and then iteratively try to
   find more starting points (x(T), λ(T), V(T)) in a way that ensures a
   desirable distribution of states at an earlier time T - ΔT. This is
   basically what Nakamura-Zimmerer et al. did.


2023-04-25

had doubts whether i am computing the right thing. went over PMP
implementation again and noticed had forgotten a -. now it looks more
plausible.

read some more papers, most I had already seen but only skimmed before,
mostly about the stochastic optimal control & neural approximations.

TODO next:

- think about a more clearly defined goal for the project?
  - simple examples and theory? GP + error bound + stability proof?
  - more applied kind of example? NN + lots of data + hyperparam opti?
  - adaptive sampling or not?
  - some of this ultra smart feynman-kac, hopf-lax type of stuff?
    - but for that i need to step up my maths game

- implement the resampling step w/ proper value function approximation

- clean up the code some more

idea for later, try this https://pypi.org/project/ppopt/ to explicitly
solve the inner minimisation over u with added input constraints? probably
nicer to have an explicit solution than cvxpy so we can still vmap, jit...

meeting questions:

- basically what is listed above
  - but also, the 'nonsmooth V' option from initial slides

- d-infk master lab extend reservation?

2023-03-26

~~~ meeting notes ~~~

show lenart resampling step.

adaptive sampling? self correcting?
forward shooting with approx V to find where we have to sample for some
desired distribution over x(0)

if always starting at T, ALL samples are actual optimal trajectories
(locally at least). If starting from some previous V approx, then errors
can accumulate.

shooting forward iteratively?

algo sketch from lenart:

    let μ = some sensible distribution over the state space
    let κ = some desired distribution of states

    loop:
        - find batch of pontryagin optimal trajectories backwards, from x(T) ~ μ
        - construct approximate V(x, t) with NN
        - simulate FORWARD from x(0) ~ κ, land at some set of x(T)
        - update μ to approximate the distribution of x(T) from previous step.

    applied iteratively, this should (with few iterations (why?)) find a
    distribution of x(T) that result in a desired distribution of x(0) or
    when travelling along pontyagin optimal trajectories.

slightly different, inspired by particle filter stuff, without forward sim:

    let μ = some sensible distribution over the state space
    let κ = some desired distribution of states

    loop:
        - find batch of pontryagin optimal trajectories backwards, from x(T) ~ μ
        - evaluate p_κ(x(0)) =: w_i for each trajectory to obtain weights
        - update μ to approximate the distribution Σ_i w_i δ(x - x_i(T))


other idea:

    have a NN ensemble/BNN/dropout net or similar for the value function,
    sample trajectories where uncertainty is high.

    could also be mixed with the oder ideas. for example:
    - simulate batch of pontryagin trajectories backwards to get x_i(0)
    - fit NN
    - find points of large uncertainty z_k
    - for each z_k: find some x_i and w_i s.t. z_k = Σ x_i w_i
    - apply the same weighted sum to the initial x(T) to hopefully get a
      trajectory landing at z_k. (this relies on some kind of local
      linearity assumption...)


to do next concretely:

- implement value function approx with simple NN
- explore sampling strategies. probably the first type of algorithm is
  easier


spent a long time watching the talks from claire tomlin "towards real-time
reachability'. basically she does numerical methods to find reachability in
differential games, i.e. robust control. they are level-set methods, i.e.
the sets are all described with functions, so for example x_unsafe = {x |
safety(x) <= 0}. that makes it possible to find backwards reachable sets
and the like.

sounds actually similar to what i am trying to do, but for state
constraints. is optimal control just a trivial byproduct of her methods?
are the level set methods basically a way of computing control barrier
functions? need to look into it more, what computations they do concretely,
at first glance it looks like they basically use a PINN style approach

would be really cool to extend the approximate optimal control stuff with
state constraints in this style, especially relating to practical
application.

implemented all the NN business logic. it now runs. but, as per usual, it
does weird stuff™. to be fixed sometime later.


2023-05-01

put nn business logic in own file, added option to automatically create
test set and show test loss. next tasks:

- include value gradient penalty.
  this should greatly improve generalisation (-> test loss) and more
  importantly fit the costate with smaller error, which is basically directly
  responsible for the control input.

- use V approximation in resampling step.

- explore sampling strategies.

interestingly, (for now with just value error loss) it seems that even
after very short time, like 1/3 epoch (with 512 trajectories over .25 sec,
leading to 3000 iterations of 128-sized minibatches). this is probably a
good thing, indicating that our data is somewhat redundant, which itself
indicates that the value function is 'nice' and we have enough data.

included option for NN test set, which is a no brainer given we have
basically infinite, clean data. implemented resampling step which actually
uses V and λ information.


2023-05-02

still same TODOs as yesterday. spent all day making the NN training faster
(much faster!) with a wider jit region, and making plotting/training
business logic a bit more bearable, as well as clearing out old resampling
code.


2023-05-03

did huge swath of refactoring. after that started implementing V gradient
penalty. it runs, but with sketchy result. TODO after lunch: debug this.
maybe start by extending the loss plots with gradient loss.

found the bug, it was both the fact that I forgot to even use the newly
defined function properly, and that it was defined with an unintended
broadcast in it, screwing the results. I think I fixed it successfully now.


2023-05-05

the current stuff works actually quite well. we get nice looking value
function (for this toy problem...), with the only problem that V seems to
become negative, indicating the approximation of V itself is not that
great, but that is not a huge problem as long as V_x is good.

however, the problem could still be that due to the repeating NN
approximation, we compound various errors (would be interesting however to
analyse to what degree this is alleviated due to the 'minimal' resampling,
where we keep the trajectories going which have not left the interesting
region).

it would be much easier if we took the route of first generating many exact
(up to numerical ODE solver errors) optimal trajectories, and then fitting
a function approximation. then however the problem becomes the selection of
terminal states/costates such that the trajectories stay in the interesting
region, and the well known problem of sensitivity growing with time horizon
applies again.

still we might alleviate this with an approach that roughly looks like
this:

1. generate optimal trajectories by simulating pontryagin backward from T,
   starting at some terminal distribution of states/costates, until like 1/2
   the trajectories have left the interesting region (or some similar
   criterion, like distribution mismatch between trajectories and desired
   such as data likelihood, KL div, ...).
   Let Termination time =: t < T.

2. take some desired distribution κ over the state space, and weight each
   trajectory by w_i = κ(x_i(t)).

3. We have a MC style approximation of the distribution of terminal states
   which leads to that desired distribution: Σ_i w_i δ(x_i(T) - x). So in
   particle filter style we can use that to generate new terminal
   conditions, by sampling from that MC distribution or a smoothed/noisy
   version of it.
   (*)

4. Repeat from 1. We have reason to believe that the integration will go
   further back than last time, because once we reach the last termination
   t, the trajectories should have our desirable distribution.

terminate once we have reached t=0 and the distribution of x_i(0) is nice


Upsides:
 - no NN in the loop
 - possible to do the 'time marching' with not too many particles, then
   when we found the suitable distribution of terminal conditions, do A LOT
   of samples, then do supervised learning only once
 - maybe easier to approximate globally optimal controls (just generate ALL
   the local optimal trajectories, select the best ones in the end), but
   maybe also more computation

Downsides:
 - probably data generation has complexity O(T^2)
     - then again generating data seems to be much cheaper than training NNs
 - hard to guarantee we will reach the desired distribution

is this just a cross entropy methdod? or a cheap version thereof?

* after thinking about it again, this is not correct. it would be if the
states x_i(t) were uniformly distributed across state space, but they're
not. still might work if the distribution is nice enough..?

this is proably what lenart & bhavya already smelled when they said we
probably still need a step of V, V_x approximation at t, sampling from κ,
simulating forward with the approximated V, THEN we actually have a
terminal state distribution which leads to the desired distribution at t.
but is it worth it? applied iteratively for time marching this seems really
inefficient. maybe as a one-time step to improve upon a value function
estimate for the whole horizon T?

actually, because the characteristic curves backwards in time kind of
diverge, it might be a workable assumption that the distribution is
uniform-ish, at least for small enough time marching intervals.... which we
could the repeat. maybe a quite efficient algorithm might be of the form:

1. find some distribution over x(T) that leads to a decent distribution
over x(0), using the time marching and importance sampling iteration
outlined above.

2. sample many trajectories, approximate V(0, x) with NN (importantly V_x(0, x))

3. do uncertainty estimation (BNN/ensemble), and find new x(T) that give
information about uncertain x(0). this should be easy as we have the many
samples already, so we only need to locally reason about the map from x(0)
to x(T). probably some approximate kernel method/weighted nearest
neighbours will do quite well. go back to 2 if not yet happy.


these kinds of approaches will probably do better (or at least, be easier)
with finite horizon and zero terminal constraint (for stability), because
then we get away with just fitting V(0, x) instead of V(t, x) and don't
have to care about the data distribution in (t, x) space, just x.



also, over lunch thought about this: the problem I had in mind for quite
some time to "fit a NN to the smallest function explained by data" can
actually be formalised quite nicely. Start from the hypothetical problem we
would like to solve:

max_θ \int c(x) f_θ(x) dx
s.t. f_θ(x_i) <= y_i   (for all i).

This assumes some class of parameterised functions f which we choose.
First, we rewrite the constraint as a penalty function, which is an
approximation but can be made arbitrarily accurate by multiplying it with a
big number λ.

max_θ [ \int c(x) f_θ(x) dx + λ Σ_i min(0, y_i - f_θ(x_i)) ]

The distribution c is arbitrary and just serves to weigh how we 'push up'
the function against the data. We might as well choose the distribution of
the data itself, as 'pushing up' the function where we have no data is
probably dumb anyways. Doing that, we end up at:

max_θ  Σ_i [ 1/N f_θ(x_i) + λ min(0, y_i - f_θ(x_i)) ]

And already we have a nice pointwise loss function suitable for machine
learning. The factors 1/N and λ can of course be combined into a single
tuning hyperparameter. Also, it might prove useful to replace the min(0, .)
by a smooth approximation.

Whether this actually works practically and how much data we need is
another question. Maybe we need some sort of regularization for it to work
as well.


back to the stuff from before. it would be really cool to unify that and
the previous approach under one algorithm. For example, the number of DP
steps (pontryagin simulation -> NN approximation) and the method used to
obtain samples from the desired distribution (None/Resampling, Time
marching & importance sampling as before) might both be put into
algo_params and throwin into some BO hyperparameter optimisation toolbox.
TODO do that next week.


2023-05-08


again thinking about methods to find pontryagin-optimal trajectories over
the whole horizon, without approximations in between. thinking about
importance sampling to find good terminal condition distribution.

basic importance sampling theory goes as follows: we have a 'difficult'
distribution q and an easier distribution p. We would like to estimate the
quantity E_{x~q} l(x)

We cannot sample from q for whatever reason, but we can evaluate its pdf up
to a normalisation factor. so instead we sample x ~ q, but weigh each
sample with the additional correction factor w = p(x) / q(x), so that
finally we replace the expectation above by E_{x~p} l(x) p(x)/q(x).

(is this basically the change of variables formula?)

However in our case we want a bit less I think, we don't need to estimate
some quantity, instead we basically have y=f(x) and want to find a
distribution over x that gives some desired distribution over y.

We can find a 'proxy' for the desired distribution. suppose we sample
x_i(T) ~ pT, then simulate backwards. The trajectories at x_i(0) will be
distributed according to some distribution p0, but we would like them to be
distributed according to q (the desired initial state distribution). To do
this we in turn have to find a better version of pT, one that ideally
achieves p0 = q.

A proposal for improvement could be the following distribution:

pT(x) <- Σ_i δ(x - x_i(T)) * q(x_i(0))/p0(x_i(0))

So basically a reweighted version of the previous terminal condition
distribution. One problem is that we don't know p0(x_i(0)) -- is it
obtainable with some change of variables formula, like multiplying pT by
the jacobian of the map from x(T) to x(0)?

basically we have x(0) = f(x(T)), where f contains the whole ODE solver
used to follow the characteristic curves, which is differentiable.

Turns out this is the good old multivariable change of variables formula.
Basically we scale the input pdf by the |(df(x)/dx)^-1|. BUT for this the
change of variables needs to be bijective, this is not given if we have
multiple locally optimal trajectories, as is usual in many interesting
cases. what happens then???

If we ditch the jacobian and assume it is 'constant enough' we can lump it
with the other scaling factors on the t=0 side. then we get more of a
heuristic method of the type "sample terminal conditions, throw away ones
we don't like, sample more of the ones we like". Which might still be
great, who knows \o/ After all the jacobian is not that informative, it
will be very large anyway, dominated by the fastest dynamics, this is
exactly the well known sensitivity thing.




---- other ways to possibly think about this problem ----

normalising flows???
cross entropy methods in optimisation (probably pretty close to above)
brute force: sample lots of x(0) and find initial costates with usual gradient methods


2023-05-10

meeting this morning. talked about new sampling-then-approximation path
which they generally like. GPs instead of NNs after all?

Spent all day trying to implement importance sampling (iteratively, with
growing horizon, so we don't have so wildly different distributions). Am
mainly fighting with diffrax's weird way of handling time steps.

write a simple own rk4 loop with fixed width????


2023-05-15

smoothed out a couple kinks in the importance sampling code. ditched
writing my own ODE integrator and wrote code that fits the design choices
of diffrax instead (thanks @Patrick Kidger!). Fixed a bug where invisible
shape-rank-promotion-broadcasting stuff was happening. it works quite well
now. also made the plotting nicer.

BUT remark: several things should still be considered.
- if we increase the time horizon, as we will have to if we want to model
  complex multiscale systems, the sensitivity problem pops up again, as we
  cannot reliably represent gaussians with very small covariance matrices.

- there are several approximations at play.
  - The distribution is 're-parameterised' as a gaussian at each iteration,
    removing the possibility of modelling multimodal distributions, which
    might well arise in practical settings (-> nonconvex decisions,
    intersecting characteristics).
  - we ditch the factor of det(jacobian_x(f(x))), where f is the function
    mapping terminal conditions to initial conditions. this means we don't
    have the actual sampling density in the importance sampling likelihood
    ratio, only something roughly proportional to it. in the (very small)
    example it works well nevertheless.

so if we want to improve it even more, basically two possibilities exist:
- make the sampling much better by modelling a wider class of distributions
  than gaussians (gaussian mixture? particle filter style?)
- go with this approach, later generate more data based on some uncertainty
  estimate
  - very natural with GPs, since they give us a) predictive uncertainty and
    b) the weights of how much each training example contributed to the
    prediction, so we basically have a nice 'approximate inverse' of the f
    mentioned before and can just say that if x0 = w.T x0_train, then
    probably also the same convex combination can be made in terms of the
    terminal conditions, together with some local linearity approximation
    at least.


TODO tomorrow:
- read up on GPs
  - jax implementations
  - conditioning on gradient observations
  - hyperparam/kernel optimisation?

- think about ways to approach stability guarantee
  - probably: bounded approximation error -> stability?
    - probably need some dubious assumptions on smoothness etc.
  - hoeffding like hertneck et al 2018? bit of a cheap way out



2023-05-16

first removed the weird (..., 1) shapes from all the code. also refactored
a bit generally. took quite long.

basic idea on a high level:
- leave sampling code as it is, it does not need to be perfect
- instead rely on GP uncertainty estimation to generate more samples as needed

so the whole algorithm could look like this:
- find initial data set of optimal trajectories using current sampling code.
- approximate (V/V_x/u) with GP (or BNN, NN ensemble)
- while (approximation not good enough):
    - Find point x with high uncertainty estimate
    - Our GP approximation looks like: E[u(x)] = Σ_i k(x, x_i) * u_i,
      where x_i and u_i are the known 'training data'.
    - We form a linear combination of terminal boundary conditions, with
      the same weights k(x_0, x_i), and hope it will give interesting new
      training data through the PMP, at some state close to x.
    - re-fit GP/etc.

thoughts on GPs:
https://github.com/JaxGaussianProcesses/GPJax
https://github.com/JaxGaussianProcesses/MOGPJax
https://proceedings.neurips.cc/paper_files/paper/2018/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf

either we use a fancy gradient-enabled GP to condition on value function
and its gradients. OR we just fit the gradients using more usual gp tools.
or even just directly the controls? we have all the data anyways... maybe
interesting to compare these options with each other...



anyways, thoughts about the software architecture side of things:

we would profit from having a sampling thing that is jitted once and can
subsequently run quickly. maybe a 'sampler' class, with methods for
a) finding suitable terminal condition distribution (what it does now)
b) only doing batched backward integration of PMP (to refine solution)

we need a GP implementation from which we can easily get the weights/kernel
matrix. probably all of them do this, should be simple. Also need a decent
way to store variable-sized data without destroying all the nice jax stuff.


2023-05-17

cool blog posts about conditioning a GP on derivative observations. seems
to be not too hard, basically if the observation is a gradient we swap the
kernel function for the gradient of the kernel function? can it really be
that simple? although it seems we make the kernel matrix much larger,
because the derivative observation is nx-dimensional, so definitely not
really scalable... but nice testbed probably.

interesting experiment would be to compare different methods of GP
regression, for example:

- fit V(x) using only value observations, compute control by argmin_u H
- fit V(x) using V & V_x = λ observations, compute control by argmin_u H
- fit λ(x) directly, compute control by argmin_u H
- compute controls by argmin_u H, fit u(x) using GP

criteria to optimise could be the test error on a holdout set, or the
closed loop control cost. also choose whether to do this with the same
dataset for each method or including some sort of uncertainty sampling.

https://gaussianprocess.org/gpml/chapters/RW.pdf, p191: 9.4 derivative observations

http://herbsusmann.com/2020/07/06/gaussian-process-derivatives/
http://herbsusmann.com/2020/12/01/conditioning-on-gaussian-process-derivative-observations/

https://blogs.ed.ac.uk/datasciencecdt/2020/08/31/derivatives-and-gaussian-processes/
https://github.com/sighellan/GP_derivatives_example/blob/master/Derivative_example.ipynb

https://docs.gpytorch.ai/en/latest/examples/08_Advanced_Usage/Simple_GP_Regression_Derivative_Information_1d.html


https://wessel.ai/stheno/docs/_build/html/readme.html

did some small experiments. basic gaussian process example, 1d, toy
function. just like in all the textbooks :)


2023-05-17

task for today: decide how to implement gp with derivative information.

a) use GP library ideally with that functionality? haven't seen one. can
   we hack our way into that functionality? basically, assign different
   kernels to different observations.

b) do it from the ground up. a very simplistic GP toolbox is basically a
   couple of manipulations on large gaussian distributions. then we could
   easily populate the K matrix with the right (value or gradient) info.


2023-05-23

finally implemented GP with derivative observation. used tinygp, a
(relatively) low level GP library, but it makes it easy to define new
kernel functions etc. in retrospect it was not too hard, just as always it
took some time to figure out which data to put where in what shape.

next steps, probably: wrap it in a class that provides high-level
operations without much boilerplate, try to use that for the 'complete'
algorithm of iteratively refining the value estimate.



2023-05-26

starting to combine the basic ingredients. idea:
- generate initial trajectory samples with pontryagin sampler
- fit GP
- iteratively sample more trajectories where uncertainty is high.

intuitively I was just going to take the uncertainty in V(x) to take more
samples. but for the controls ultimately, grad_x V(x) is relevant. granted
when we have samples of V(x) and V_x(x) at the same locations it probably
won't matter too much. but still, we should think about how to adaptively
sample if we decide to adaptively sample.

the other approach would be to use a better sampling method in the first
place which guarantees that we actually reach some desired distribution.
but that is probably a bit harder...

also tried to implement a simple approach where after finding some new
states with uncertain V estimate, we try to 'predict' which terminal
conditions can generate new data there, by basically reusing the GP that
predicts V from x(0) to predict λ(T) from x(0).



2023-05-29

tried a bit more to do the last thing, turns out it is not so easy, if I
manually apply the gaussian conditioning formula with (I think) the right
kernel matrices inside, I still get different predictions than with the GP.
In any case, a method like this would not scale to NNs at all. Maybe it
would be easier to do something like this:

sample trajectories, fit V

loop:
  sample new trajectories
  subloop:
    adjust the terminal conditions in direction of ascending uncertainty.
  add these new trajectories to the training set


this could easily be done with GPs and NNs, probably by connecting it to
the right theory in the realm of sampling algorithms etc. we can show that
it is an approximation of sampling from a distribution proportional to
exp(uncertainty estimate), a boltzmann-ish distribution.

problems could be the sensitivity problem again - probably difficult to
tune gradient descent algorithms. maybe if we do some PCA-like
transformation at the input?

or ditch gradient altogether and do everything as a sampling-type
algorithm? as in, sample new trajectories and then run a evolutionary
algorithm kind of thing to arrive at a set of terminal conditions with high
value estimate uncertainty...

generally I see a couple different flavours of this kind of stuff:

1. separately maximise uncertainty in terms of x(0) directly, and then
   search for terminal conditions that give information there.

2. maximise uncertainty as a function of x(T). This would be the nicest,
   but maybe leads to sensitivity problems etc. but with the right tricks
   it might work. i am thinking of
   a) reparameterising the input space by pre-scaling Σ^(1/2), the sqrtm
   covariance of the terminal condition distribution found by the
   pontryagin sampling code
   b) trust region.

3. maximise uncertainty in terms of x(0) as well, but after each gradient
   step, estimate a new terminal condition (probably w linearization), and
   simulate from there to get next 'actual' x(0).

probably 3 is most promising? at least in terms of immediate practicality.
on the other hand I feel like 2 is the one which would lead to the best
scalability.

decision for now: investigate 2. seems the nicest in terms of theory,
probably more computation than strictly necessary, but it is offline so who
cares \o/ still better than RL hehe


2023-05-30

did some small steps, first implementation of the complete algorithm is
running, very small example and GP. it goes like this:

generate initial set of PMP-optimal trajectories with importance sampling
fit GP
loop:
    generate new terminal conditions from same distribution used initially
    take N gradient steps adjusting x(T)/λ(T) in direction of ascending value uncertainty
    re-fit GP with that data

probably this can nicely be explained theoretically as a sampling algorithm
(MCMC/SGLD style) that samples from a distribution concentrated around
regions (in terminal condition space) that lead to uncertain value function
estimate in x(0) space. this in turn means we should be able to 'cover' the
complete interesting state space at t=0, because, for example:

- the interesting region is contained within the set for which optimal
  trajectories towards the goal exist
- the sampling algorithm (in theory) can explore ALL of x(T)/λ(T), so for
  every interesting initial condition there will be a good value estimate
  eventually.

or, ditching the first assumption, then we get at each initial state x(0)
one of two outcomes:
- either the sampling algorithm eventually finds terminal conditions that
  inform the value function at x(0), to some desired accuracy
- or there exists no such terminal condition, which will lead to large
  uncertainty at x(0), so with a sublevel set of the model uncertainty we
  get an estimate of the stabilizable set as well :)

this looks actually promising, many practical things can be addressed
nicely like this. for example, the interesting region can be added as a
penalty term which decreases likelihood significantly, making the sampling
algorithm naturally 'uninterested' in regions outside the pre-defined
interesting region, which is not given in the current mockup
implementation.

furthermore, (i think that) nonconvexities/multiple locally optimal
solutions could be handled gracefully. by defining some desired
distribution over the state space at t=0, we also define a distribution
over x(T)/λ(T), but not in the obvious sense - if there exist different
local solutions starting at some x(0) then the corresponding probability
density will show up twice in the terminal condition space. this causes
issues with normalization, probably one of them will have to stop being a
proper probability density function, just some sort of handwavy analogue,
but that should not stop us from trying to sample from it. but probably not
in this semester project...

also, the distributions can also be quite ugly for this type of algorithm,
which is exactly what will happen once we try to include state constraints,
which will introduce additional costate-like lagrange multipliers which
probably will need to be found at T to produce optimal controls at some
state at t=0. but surely not in this semester project :o
