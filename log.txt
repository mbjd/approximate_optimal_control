2024-01-15

decided to use this log again for daily stuff. 

looked into value expansion version again, from DDP type solvers in continuous
time. https://dcsl.gatech.edu/papers/acc15e.pdf here is a nice derivation for
the HJI equation. 

also debating: if we go with this value expansion option, we will have to do
more computation per trajectories, the state is enlarged by an (nx, nx) matrix
(symmetric so maybe half that). is it worth it? can we effectively explore the
state space without too much unnecessary data? 

thinking a lot about the "fundamental limits" of this purely backward-in-time
way of solving the problem. inevitably the bottleneck will be travelling along
long, turnpike-type trajectories, when the fast dynamics, which are unstable in
backward time, constantly push us away from it. Will we actually end up
creating much more data than what would be needed? Would it be a smart(ish)
idea to throw away *some* of the trajectories to limit dataset size? 

also, inuitively, it seems like higher dimensional problems are even harder
from this perspective. If we want to re-start HJB characteristics away from the
terminal set, we need accurate knowledge of λ(x) and V(x) at that point. This
in turn we can only do when the data density in that region is alredy high, so
that either some nearest neighbor interpolation or taylor extrapolation is
accurate enough. But does this even work in high dimensions? Basic "curse of
dimensionality" intuition says that as dimension increases, this type of
nearest neighbor search becomes increasingly a bad idea: The dataset size
needed to satisfy some fixed data density is exponential in dimensionality. 

(another interesting question here would be to challenge the assumption that we
need accurate V and λ to continue a HJB characteristic curve. If λ is slightly
wrong, can we prove something about the suboptimality of the "stitched"
characteristic curve w.r.t. the actual optimal trajectory?)

In the same vein, my initial assumption kind of has been: Better to "stitch
together" optimal trajectories by re-initialising the characteristic ODE (a),
than to re-calculate the whole final part of the trajectory (b). Is this true
tough? To do (a), we have to interpolate the costate and value info, which in
higher dimensions might be problematic. (and even if the sets from which most
backward trajectories start locally resemble a lower-dimensional subspace,
interpolation requires lots of care to not go "outside" of that subspace too
much. This is also something I think where quadratic value expansions can help
a lot: the hessian of the value gives us a nice distance metric, which maybe
encapsulates this "lower dimensional turnpike subspace" thing which hopefully
exists)

all of this makes me ask the question: would a DDP type solver be a better
overall fit? the benefit of our approach (hopefully) is that we only generate
(locally) optimal trajectories, wasting no time on iteratively adjusting
suboptimal ones. However, that advantage can be nullified if in the process of
exploring the state space with optimal trajectories, we generate much more of
them than what we would need for training an NN. Maybe we could have made a
similarly good dataset with much fewer DDP-generated optimal trajectories, even
if they are a bit more expensive to calculate, but we only generate the amount
actually needed, which probably is a lot less than the current idea. 

this makes me heavily question the purpose of the project. if we just take a
DDP solver, why not one of the 100s that already exist? If we do our
characteristics type thing we can already guess the conclusion*, so why do it?  

* something like:

we attempt to solve control problems to global optimality by evaluating optimal
trajectories backwards. end up with no global optimality guarantee, best case
some probabilistic asymptotic approximate thing. It works great for
"non-multiscale", low-ish dimensional problems, for others we end up generating
huge datasets for not much tangible benefit, all while algo complexity is
basically O(dataset size^2). 

ways to address this: 
- be aware that it probably won't be state of the art, have fun exploring 
  algo details and heuristics to speed it up and stuff. 
- connect with previously existing methods (ddp) and work from there to some 
  sort of global optimality guarantee
- do a 180 and start a somewhat different project? (PINN? active exploration?)

also still kind of debating thoughts from last week: should we go all in on
making it efficient with the right data structures? store trajectories in a
tree or DAG like thing based on "flow" of information? use some black magic to
speed up nearest neighbor search, like k-d tree or locality sensitive hashing?
because pretty much no matter how we go about it, we will not only create a
sizable data set of trajectories, but also have to do many, many
nearest-neighbor-type searches over it. 

this was a lot of maybe incoherent rambling. sorry to my future self and
whoever reads this. maybe the final paper will be more readable \o/



2023-01-16

looked through literature. found out that our gradient enabled neural network
is already known under one more name: Sobolev training (sobolev spaces are
function spaces endowed with a norm that looks like a p-norm of a vector of
function norms of all partial derivatives up to order k). specifically there is
also an interesting phd thesis:
https://gepettoweb.laas.fr/articles/amit_icra_22.html. They basically use a DDP
type solver coupled with global value function approximation, to find infinite
horizon optimal control over a large domain by iteratively using the learned
value function as terminal cost. 

Should we try to expand this to some sort of provable global optimality?
Because I don't think they do that there. There the claim "So, when used as a
proxy for terminal cost functional, ∂PVPv [name of their algo] tends to drive
the locally optimal solver toward the globally optimal solution.", but no proof
or explanation. 

possible decent contribution would also be to do the infinite horizon in one
single trajectory with some sort of time reparameterisation. reduces the number
of approximations and weak links, and I think with adaptive solver this can
actually be quite efficient. In fact, I think inf-horizon DDP itself would already 
be a cool contribution, even without the "hopefully global" optimality. 

other new interesting papers in semanticscholar folders: 
- ddp-ish value expansion
- local -> global optimality
- sobolev learning
- V approx, transfer learning, etc. 


small tidbits i thought about: 

- in continuous time DDP apparently the hessian Vxx is always positive
  definite, in contrast to the more often used time-discretised version. Does
  this hold even when we have a really sloppy numerical solver? would be cool if
  yes

- NN learning: only consider data points with v(x) <= v, and sweep up v during
  training? this way we might explicitly identify the points when different
  local solutions start conflicting, and maybe do something about it. 


2023-01-17

had meeting. talked about all the concerns from last few days and got
reassuring answer that yes, these are hard and, in practical terms, unsolved
problems. advice: just explore around for another week and slowly try to come
up with a sensible research question. 

mentioned frustration that "everything" has already been done, no "niche" left.
answer: "own" niche can also be very small, or a contribution to extend
existing work can also be quite small to be significant. 

current options: 

- explore (probably limited) possibilities with pure PMP backward integration. 
  also sometimes labelled geodesic flow, geodesic spray, etc. probably only
  simple systems, low dim, not multiscale. 

- explore backwards PMP integration + quadratic value expansion. the appeal is
  obviously still no iterative optimisation over trajectories/inputs. Maybe we 
  can slightly improve the problem of backward time instability leading to way 
  more data than we actually need? Probably, but scalability concern remains. 

- go for a DDP like solver method, similar to PhD thesis of Amit Parag, and try
  to come up with tricks to "encourage" global optimality. Lots of ideas around
  BNNs with asymmetric loss, Sobolev learning, active learning, etc. currently
  this does seem the most interesting to me. 

- sth else? 

different, mostly-orthogonal directions to expand the problem setting beyond
inf-horizon, continuous time: 

- some sort of approximate *global* optimality. probably though there just
  isn't an actual shortcut to this. give up? 
- input constrained (in general dims -> QP solver finds u*)
- state constrained (in the sense of safety limits) 
- state constrained (in the sense that x is on a manifold)
- robust (with adversary, HJI equation)
- parameterised family of dynamics


other than that: 

- devoured more literature about continuous-time DDP-ish solvers, especially
  interesting ones to do w/ HJI/robust stuff. 

- found a nice paper comparing different BNN methods to the "true" posterior
  obtained with Hamiltonian Monte Carlo, put it in a new BNN folder. 

- found a couple more papers about what I think is essentially the same as
  pontryagin backward integration like in SA, one example was planning min-time
  flight trajectories on the actual globe with known wind field and simplified
  single integrator dynamics. that is actually a cool application, where the
  multiscale limitations do not come into play. 

Random assorted thoughts, about making DDP work in inf-horizon:

I am doubtful as to whether the time-value rescaling, or some different time
rescaling, is needed. Probably the adaptive solver will be just as efficient
without it [citation needed]. What is more interesting then is how do we choose
the time horizon. Simple first idea: just choose a large-ish time horizon and
define a terminal set Xf (eg. LQR value sublevel set). Then, optimise
trajectories using DDP. If the trajectory ends inside of Xf, good. If not,
increase the time horizon. In continuous time and w/ adaptive solvers, [i hope
that] a "too" long time horizon will not be much of an issue, especially if
most of the time is spent within Xf, where the ODE solver can choose large
steps. 

And about distributions/ill conditioning/timescale separations: 

also, many of the problems we see (ill conditioning, bad distributions) arise
from basically the fact that systems are decomposable across time scales.
Traditionally, this makes engineering solutions easier, however if we aim to
solve the whole problem at once it becomes much more difficult. Can we somehow
use these time-scale decomposition heuristics to maybe improve/inform our
sampling process, while still ultimately finding the actual solution to the
full problem? not so sure how exactly to go about this. 
