2023-03-13

the great start. debated long whether to use equinox or flax. lenart
recommends/uses flax. but the basic example seemed really convoluted, as
did the syntax for creating NNs. I think I will use equinox, it seems more
minimalistic while also requiring less boilerplate code (it seems), and the
complete compatibility with jax functions seems nice.

probably doesn't matter much either way, both libraries are quite small and
most of the boilerplate is a decision of the developer (how to handle
state and stuff).

am coding a small supervised learning example with a toy dataset with
equinox.

it works! am trying out lots of different optimizers from optax and
making 3d plots of the test error depending on training iteration and
learning rate. just to get a feel for things :)

adam, adabelief and yogi seem very nice for lr around 10^(-2.2) to
10^(1.4). adam probably the most robust so not much need to play around.

also did some experiments with continously sampled training data, still
supervised though. there is an interesting thing happening when we change
batch sizes. smaller batch sizes are more efficient per data point, but
incur more overhead in the training loop. would be interesting to do an
experiment where we plot the test error depending on wall training time to
find the practical optimum.

or, practically for now, find some settings by hand that work, and then in
the end throw all the hyperparams into some bayesian optimisation thing
that tries to achieve minimum test error in some given (wall) time frame.

good first day. learned a lot, gained confidence, did not get bored from
9:30 to 17:45.

2023-03-14

pivoted from optimizer tuning to batch size tuning and making nice plots.
it seems that w.r.t. wall time, batch sizes of 256-1024 are in the sweet
spot, with larger batch sizes being worse initially but better if trained
longer.

also did some nice 3d visualisations of a network training.

Noticed that I have a NN of the form y = x.T @ x, where x is the output of
the last layer. If instead we append a general layer to the NN somehow it
stops working, and somehow fits an almost constant function. TODO find out
why.

spent most of the afternoon debugging that, did not get far. instead only
found confusing stuff.

when the NN returns the squared norm of last layer activation (the
situation where everything works quite well) the gradient norm keeps
increasing during training, even though the loss decreases to 0.1 or 0.001
which is low enough according to the plot. the individual gradients look to
be mostly less than 0.2 in magnitude, with the largest ones growing a bit
above 1.

with the other 'standard' NN configuration (linear last layer with scalar
output), the test loss quickly stabilises quite exactly to about 5, with
the monte-carlo training loss between 6 and 11. here, the gradient norm
does decrease, but only very slowly, and the fit is poor.

made some plots to show lenart tomorrow.







2023-02-15

meeting w/ lenart and bhavya

supervised learning to approximate mpc? -> practical solution.

ct: value iteration? soft actor critic?


lenart's research.
learn probabilistic model of dynamics.
dt: q function. ct: problem bc u no influence on 'next' value. BUT value
derivative. this is where hjb comes from. value function does not depend on
u.


plan: formulate possible paths to take:
- hjb solution with known dynamics f
    this they kind of discourage bc. we need to "learn" the model f, and if
    we know f already then approximated MPC is the practical answer.
- approximated mpc
    practical approach to the problem at work, also interesting things to
    do with uncertainty sampling/active learning, stuff like that.
- value function estimate directly from sim data (red on whiteboard)
    this is basically RL.
- stuff on left of whiteboard

dt/ct which route to take? they say dt is probably easier. ct seems kind of
cleaner though. dunno

also TODO:
- read jax tutorial lenart sends
- read papers lenart sends
- get an overview of classic RL algorithms
- read DPOC notes, specifically the leap from discrete to continous time.

next week 9-10.



2023-03-21

reading some more papers. read three yesterday, was too much, head was
full.

did some small experiment, with solving the simplest possible PDE
(basically grad_x f(x) = 2x, with solution x.T x). it seems to scale
not-very-well with the dimension of the problem... but also, hyperparams
are quite crucial. it seems like small networks already do quite well, and
the common wisdom of large nets being simpler to train does not seem to
apply here. tried some decreasing learning rate stuff, worked really
nicely.

I am thinking that what lenart and bhavya suggest, i.e. going the RL-route
to construct a control policy from sim data, is probably a more fruitful
project.

as for pure practicality, they suggest (at work) that MPC -> NN
approximation is probably the way to go, which I also think is right.

possible roadmaps:

1.  do a 'regular' RL-type project, going from sim data to control policy,
    certainly also towards the computationally easier side of RL problems.

    fundamental question: how do we go from sim data to approximately optimal
    control policy, without writing down the model (again)?  maybe focus on
    data efficiency, safety/verifiability, something like that?

1a. do a RL-type project, but focus on making *some* aspect of it continuous
    time? the stuff they talked about basically.

2.  go all in on the pde-type approach, do hyperparam optimisation, find
    scaling laws etc. more exploratory than immediately practical.

    fundamental question: how can 'deep galerkin methods' be made practical
    in an optimal control context? how do we find a NN description of
    optimal value function or similar, WITH availability of a continuous-time
    model?

3.  do the ultra-practical option of MPC -> approximate control inputs with
    NN. surely the easiest in terms of extra engineering efforts, but also
    not many interesting results probably? do that at work sometime :)

    explore further options: theoretically sound, investigate the error, do
    robust mpc to start with. scalability? error bound with hoeffding eq.




2023-03-22

{{{
talk with bhavya and lenart.

bhavya concerns about possibility to deliver? decide on something to
investigate? -> do not rush, even 'non-results' are results, they are
basically open to everything, but want me to know what to expect.


hjb, model based approach theoretically quite clear. certainly interesting
and feasible, even if not very practical immediately.


also very exploratory ideas, which they brainstormed about quite long.

cont time rl ideas... all with parameterized policy: they see some
difficulties. lots of questions about how exactly to interface continuous
<-> discrete time, bc. measurements are only available in discrete time
anyway... something like discrete reward measurements -> estimate
continuous time optimal V, run controller in ct (faster?) lots of
interesting questions, lots of complications.

Q function in terms of x, u, and Δt? include Δt in the action space?
only change u a couple of times over time horizon? optimize over timing of
u changes? advantage of changing controller in reward somehow?
variable-rate control? very crazy... they throw around lots of ideas and I
understand probably half of it. cool idea, lots of missing pieces.

maybe ct rl is just not worth it \o/

do the decision whenever I want to and like to. sign up on mystudies once
the roadmap is clear, in 3-4 weeks they would start to "ask again" whether
I signed up.
}}}



did some more experiments. basically only hyperparam tuning on quadratic
pde example. learning rate schedules, network sizes, 'loop unrolling' with
multiple gradient steps in jit region.

lr schedule has big impact and is probably worth putting in some bayesian
hyperparam optimisation thing. network sizes maybe too, but mainly
influence computation time (still interesting if hyperparam opti is w.r.t.
fixed wall time.) unrolling probably not worth it, small speedup only but
complications with training indices and lr schedules.

todo, maybe also at work sometime: try jax for trajectory optimisation
(and everything else?)
https://github.com/google/trajax
https://github.com/nikihowe/myriad


2023-03-27

small experiments with training hyperparams. still just grad f(x) = 2x PDE.
jax.lax.fori_loop makes a multiple-gradient-step much nicer, fast jit, now
it is actually worth it.


2023-03-28

read some more papers, they are all in the appropriate semanticscholar dir.

some thoughts on the different options for the projects.

1. (classic RL)
   lots of examples & research
   highly practical as well
   good area to get into hypothetically

1a. (RL but focus on continuous time)
   some theoretical challenges, question practicality

2. pure PDE solver type approach
   very non-practical probably
   some possible upsides to other methods?
    - better handle on nonconvexity than predictive control
    - also nonconvex constraints - but constraints generally a bit harder
      again...

3. approximate MPC
   probably boring, very practical, basically done by
   Hertneck/Köhler/Trimpe/Allgöwer 2018. Neural&stochastic pendant of
   explicit MPC.



tbh, probably it will still be 2.

stuff to do:

- literature review & define goals for experiments
  possible options:
  - parameterisations of V
  - play around with different loss formulations
  - random vs adversarial sampling - uncertainty sampling? -> easier problem probably they say
  - go a bit into theory about nondifferentiability/viscosity? --> probably hard/unpractical they say
  - find connections to lygeros 'find largest V such that V <= TV' idea from the ADP paper?
  - principled, end-to-end hyperparam search?
    (optimising for MC control cost w/ fixed training time)
  - do something with state constraints? growing outer or shrinking inner penalty function? explicit lagrangian multiplier by another NN?
    safety verification type stuff...
  - investigate connection to various RL algorithms?
- experiment
- write


basically two broad directions:
- more theoretical (infinite-dim optimization with NN, viscosity solutions, etc)
- more practical (make it work with ML tricks)



we have 3 months basically. possible roadmap? to be adjusted continuously :)

months from start (30d each)
|------------------------------|------------------------------|------------------------------|
weeks (7d)
|------|------|------|------|------|------|------|------|------|------|------|------|------|------
^             ^             ^                                  ^             ^
|             |             |                                  |             start ONLY writing
|             |             |                                  finalize experiments (collect data, make plots, ...)
|             |             start experiments (adapt if too long/elaborate :)
|             start defining experiments?
literature review



2023-03-29

continue:
literature review about interesting stuff
present: state of the art and direction to investigate more. slides/whiteboard/whatever

try https://www.notion.so/


so, literature review time.

reading list for next time:

https://arxiv.org/pdf/2302.13122.pdf,
https://arxiv.org/pdf/2302.09878.pdf, and
https://arxiv.org/pdf/2002.08625.pdf but only skim, very math heavy

https://arxiv.org/pdf/2102.11802.pdf some neural pde stuff overview
dpoc script - hjb eq again.

one or two simpler ones from the direct hjb solving folder.



2023-04-04

am quite optimistic atm about an approach i thought about for work once.
basically, instead of directly taking the hjb eq., we start from the
pontryagin principle, but solve it backwards. so instead of the usual
problem (search for λ(0) leading to the desired x(T)) we have the reverse
-- start at some x(T) and integrate the adjoint system backwards, so we
arrive at lots of (state, value) pairs.

the good thing is we don't have to search for some exact initial state by
guessing the correct final state -- we want ALL the initial states. so
there is bound to be some sort of MC+uncertainty sampling method that does
quite well. the backwards adjoint system is probably unstable though so
have to watch out for that. probably we end up with yet another MC pde
solver with adaptive resampling. which is nice after all :)

found out that this is probably a method of characteristics:
https://en.wikipedia.org/wiki/Method_of_characteristics

we are bound to run into instability problems with this method. the
(optimal) forward system is stable, so the backward one will be unstable in
all dimensions, so the fastest dynamics will mess us up first.

can be addressed by various methods.

- iterative backward sim - function fitting thing, akin to backward DP
- completely adaptive starting and stopping of characteristics with some
  form of uncertainty sampling
- dont use pontryagin but nonlinear mpc
  together with many x0 sampled over state space, maybe with SGLD to only
  travel small amounts for good initialization, and a method to fit 'the
  lowest plausible value function' (see lygeros ADP paper - largest
  underestimator), we might still get some form of global optimality

now, i need to make slides and for that i need to tell some kind of story.
i would love to start with the control/rl comparison and then show how i am
exploring a new-ish niche in between the two -- solving control people's
problems using ML tools.


2023-04-23

https://www.youtube.com/playlist?list=PLHyI3Fbmv0ScvcPAT9IK6YPEZGnj-6Sqk


2023-04-24

setup eth workstation. did everything with virtualenv, very nice, only had
to fiddle a bit to get matplotlib to use a working GUI backend.

implemented resampling step properly (well, without function approximation,
so it still does nothing). but the business logic around that is nice and
works. atm I have a condition where we resample the sample that have a
mahalanobis distance >= some threshold from the initial state distribution.

maybe trying to resample as few trajectories as possible is not smart.
ranted we get the benefit of longer periods of 'exact' backwards ODE
integration, BUT the distribution of states may become undesirable,
practically it often becomes a distribution with no samples in large
regions and many samples in some regions. this might be fixed by one of two
things:

 - complete resampling after only a relatively short time (i.e. such that
   the distribution of states does not qualitatively change a lot - maybe
   also triggered by some condition relating distribution mismatch between
   sampling distribution and actual distribution of trajectories, like data
   likelihood or KL divergence). then we always stay at the sampling
   distribution and can nicely control which states are visited, BUT incur
   extra error due to the frequent alternation between sample-based and
   function-based representations.

 - some smarter thing where we first simulate a small-ish amount of
   pontryagin optimal trajectories backwards, and then iteratively try to
   find more starting points (x(T), λ(T), V(T)) in a way that ensures a
   desirable distribution of states at an earlier time T - ΔT. This is
   basically what Nakamura-Zimmerer et al. did.


2023-04-25

had doubts whether i am computing the right thing. went over PMP
implementation again and noticed had forgotten a -. now it looks more
plausible.

read some more papers, most I had already seen but only skimmed before,
mostly about the stochastic optimal control & neural approximations.

TODO next:

- think about a more clearly defined goal for the project?
  - simple examples and theory? GP + error bound + stability proof?
  - more applied kind of example? NN + lots of data + hyperparam opti?
  - adaptive sampling or not?
  - some of this ultra smart feynman-kac, hopf-lax type of stuff?
    - but for that i need to step up my maths game

- implement the resampling step w/ proper value function approximation

- clean up the code some more

idea for later, try this https://pypi.org/project/ppopt/ to explicitly
solve the inner minimisation over u with added input constraints? probably
nicer to have an explicit solution than cvxpy so we can still vmap, jit...

meeting questions:

- basically what is listed above
  - but also, the 'nonsmooth V' option from initial slides

- d-infk master lab extend reservation?

2023-03-26

~~~ meeting notes ~~~

show lenart resampling step.

adaptive sampling? self correcting?
forward shooting with approx V to find where we have to sample for some
desired distribution over x(0)

if always starting at T, ALL samples are actual optimal trajectories
(locally at least). If starting from some previous V approx, then errors
can accumulate.

shooting forward iteratively?

algo sketch from lenart:

    let μ = some sensible distribution over the state space
    let κ = some desired distribution of states

    loop:
        - find batch of pontryagin optimal trajectories backwards, from x(T) ~ μ
        - construct approximate V(x, t) with NN
        - simulate FORWARD from x(0) ~ κ, land at some set of x(T)
        - update μ to approximate the distribution of x(T) from previous step.

    applied iteratively, this should (with few iterations (why?)) find a
    distribution of x(T) that result in a desired distribution of x(0) or
    when travelling along pontyagin optimal trajectories.

slightly different, inspired by particle filter stuff, without forward sim:

    let μ = some sensible distribution over the state space
    let κ = some desired distribution of states

    loop:
        - find batch of pontryagin optimal trajectories backwards, from x(T) ~ μ
        - evaluate p_κ(x(0)) =: w_i for each trajectory to obtain weights
        - update μ to approximate the distribution Σ_i w_i δ(x - x_i(T))


other idea:

    have a NN ensemble/BNN/dropout net or similar for the value function,
    sample trajectories where uncertainty is high.

    could also be mixed with the oder ideas. for example:
    - simulate batch of pontryagin trajectories backwards to get x_i(0)
    - fit NN
    - find points of large uncertainty z_k
    - for each z_k: find some x_i and w_i s.t. z_k = Σ x_i w_i
    - apply the same weighted sum to the initial x(T) to hopefully get a
      trajectory landing at z_k. (this relies on some kind of local
      linearity assumption...)


to do next concretely:

- implement value function approx with simple NN
- explore sampling strategies. probably the first type of algorithm is
  easier


spent a long time watching the talks from claire tomlin "towards real-time
reachability'. basically she does numerical methods to find reachability in
differential games, i.e. robust control. they are level-set methods, i.e.
the sets are all described with functions, so for example x_unsafe = {x |
safety(x) <= 0}. that makes it possible to find backwards reachable sets
and the like.

sounds actually similar to what i am trying to do, but for state
constraints. is optimal control just a trivial byproduct of her methods?
are the level set methods basically a way of computing control barrier
functions? need to look into it more, what computations they do concretely,
at first glance it looks like they basically use a PINN style approach

would be really cool to extend the approximate optimal control stuff with
state constraints in this style, especially relating to practical
application.

implemented all the NN business logic. it now runs. but, as per usual, it
does weird stuff™. to be fixed sometime later.


2023-05-01

put nn business logic in own file, added option to automatically create
test set and show test loss. next tasks:

- include value gradient penalty.
  this should greatly improve generalisation (-> test loss) and more
  importantly fit the costate with smaller error, which is basically directly
  responsible for the control input.

- use V approximation in resampling step.

- explore sampling strategies.

interestingly, (for now with just value error loss) it seems that even
after very short time, like 1/3 epoch (with 512 trajectories over .25 sec,
leading to 3000 iterations of 128-sized minibatches). this is probably a
good thing, indicating that our data is somewhat redundant, which itself
indicates that the value function is 'nice' and we have enough data.

included option for NN test set, which is a no brainer given we have
basically infinite, clean data. implemented resampling step which actually
uses V and λ information.


2023-05-02

still same TODOs as yesterday. spent all day making the NN training faster
(much faster!) with a wider jit region, and making plotting/training
business logic a bit more bearable, as well as clearing out old resampling
code.


2023-05-03

did huge swath of refactoring. after that started implementing V gradient
penalty. it runs, but with sketchy result. TODO after lunch: debug this.
maybe start by extending the loss plots with gradient loss.

found the bug, it was both the fact that I forgot to even use the newly
defined function properly, and that it was defined with an unintended
broadcast in it, screwing the results. I think I fixed it successfully now.


2023-05-05

the current stuff works actually quite well. we get nice looking value
function (for this toy problem...), with the only problem that V seems to
become negative, indicating the approximation of V itself is not that
great, but that is not a huge problem as long as V_x is good.

however, the problem could still be that due to the repeating NN
approximation, we compound various errors (would be interesting however to
analyse to what degree this is alleviated due to the 'minimal' resampling,
where we keep the trajectories going which have not left the interesting
region).

it would be much easier if we took the route of first generating many exact
(up to numerical ODE solver errors) optimal trajectories, and then fitting
a function approximation. then however the problem becomes the selection of
terminal states/costates such that the trajectories stay in the interesting
region, and the well known problem of sensitivity growing with time horizon
applies again.

still we might alleviate this with an approach that roughly looks like
this:

1. generate optimal trajectories by simulating pontryagin backward from T,
   starting at some terminal distribution of states/costates, until like 1/2
   the trajectories have left the interesting region (or some similar
   criterion, like distribution mismatch between trajectories and desired
   such as data likelihood, KL div, ...).
   Let Termination time =: t < T.

2. take some desired distribution κ over the state space, and weight each
   trajectory by w_i = κ(x_i(t)).

3. We have a MC style approximation of the distribution of terminal states
   which leads to that desired distribution: Σ_i w_i δ(x_i(T) - x). So in
   particle filter style we can use that to generate new terminal
   conditions, by sampling from that MC distribution or a smoothed/noisy
   version of it.
   (*)

4. Repeat from 1. We have reason to believe that the integration will go
   further back than last time, because once we reach the last termination
   t, the trajectories should have our desirable distribution.

terminate once we have reached t=0 and the distribution of x_i(0) is nice


Upsides:
 - no NN in the loop
 - possible to do the 'time marching' with not too many particles, then
   when we found the suitable distribution of terminal conditions, do A LOT
   of samples, then do supervised learning only once
 - maybe easier to approximate globally optimal controls (just generate ALL
   the local optimal trajectories, select the best ones in the end), but
   maybe also more computation

Downsides:
 - probably data generation has complexity O(T^2)
     - then again generating data seems to be much cheaper than training NNs
 - hard to guarantee we will reach the desired distribution

is this just a cross entropy methdod? or a cheap version thereof?

* after thinking about it again, this is not correct. it would be if the
states x_i(t) were uniformly distributed across state space, but they're
not. still might work if the distribution is nice enough..?

this is proably what lenart & bhavya already smelled when they said we
probably still need a step of V, V_x approximation at t, sampling from κ,
simulating forward with the approximated V, THEN we actually have a
terminal state distribution which leads to the desired distribution at t.
but is it worth it? applied iteratively for time marching this seems really
inefficient. maybe as a one-time step to improve upon a value function
estimate for the whole horizon T?

actually, because the characteristic curves backwards in time kind of
diverge, it might be a workable assumption that the distribution is
uniform-ish, at least for small enough time marching intervals.... which we
could the repeat. maybe a quite efficient algorithm might be of the form:

1. find some distribution over x(T) that leads to a decent distribution
over x(0), using the time marching and importance sampling iteration
outlined above.

2. sample many trajectories, approximate V(0, x) with NN (importantly V_x(0, x))

3. do uncertainty estimation (BNN/ensemble), and find new x(T) that give
information about uncertain x(0). this should be easy as we have the many
samples already, so we only need to locally reason about the map from x(0)
to x(T). probably some approximate kernel method/weighted nearest
neighbours will do quite well. go back to 2 if not yet happy.


these kinds of approaches will probably do better (or at least, be easier)
with finite horizon and zero terminal constraint (for stability), because
then we get away with just fitting V(0, x) instead of V(t, x) and don't
have to care about the data distribution in (t, x) space, just x.



also, over lunch thought about this: the problem I had in mind for quite
some time to "fit a NN to the smallest function explained by data" can
actually be formalised quite nicely. Start from the hypothetical problem we
would like to solve:

max_θ \int c(x) f_θ(x) dx
s.t. f_θ(x_i) <= y_i   (for all i).

This assumes some class of parameterised functions f which we choose.
First, we rewrite the constraint as a penalty function, which is an
approximation but can be made arbitrarily accurate by multiplying it with a
big number λ.

max_θ [ \int c(x) f_θ(x) dx + λ Σ_i min(0, y_i - f_θ(x_i)) ]

The distribution c is arbitrary and just serves to weigh how we 'push up'
the function against the data. We might as well choose the distribution of
the data itself, as 'pushing up' the function where we have no data is
probably dumb anyways. Doing that, we end up at:

max_θ  Σ_i [ 1/N f_θ(x_i) + λ min(0, y_i - f_θ(x_i)) ]

And already we have a nice pointwise loss function suitable for machine
learning. The factors 1/N and λ can of course be combined into a single
tuning hyperparameter. Also, it might prove useful to replace the min(0, .)
by a smooth approximation.

Whether this actually works practically and how much data we need is
another question. Maybe we need some sort of regularization for it to work
as well.


back to the stuff from before. it would be really cool to unify that and
the previous approach under one algorithm. For example, the number of DP
steps (pontryagin simulation -> NN approximation) and the method used to
obtain samples from the desired distribution (None/Resampling, Time
marching & importance sampling as before) might both be put into
algo_params and throwin into some BO hyperparameter optimisation toolbox.
TODO do that next week.


2023-05-08


again thinking about methods to find pontryagin-optimal trajectories over
the whole horizon, without approximations in between. thinking about
importance sampling to find good terminal condition distribution.

basic importance sampling theory goes as follows: we have a 'difficult'
distribution q and an easier distribution p. We would like to estimate the
quantity E_{x~q} l(x)

We cannot sample from q for whatever reason, but we can evaluate its pdf up
to a normalisation factor. so instead we sample x ~ q, but weigh each
sample with the additional correction factor w = p(x) / q(x), so that
finally we replace the expectation above by E_{x~p} l(x) p(x)/q(x).

(is this basically the change of variables formula?)

However in our case we want a bit less I think, we don't need to estimate
some quantity, instead we basically have y=f(x) and want to find a
distribution over x that gives some desired distribution over y.

We can find a 'proxy' for the desired distribution. suppose we sample
x_i(T) ~ pT, then simulate backwards. The trajectories at x_i(0) will be
distributed according to some distribution p0, but we would like them to be
distributed according to q (the desired initial state distribution). To do
this we in turn have to find a better version of pT, one that ideally
achieves p0 = q.

A proposal for improvement could be the following distribution:

pT(x) <- Σ_i δ(x - x_i(T)) * q(x_i(0))/p0(x_i(0))

So basically a reweighted version of the previous terminal condition
distribution. One problem is that we don't know p0(x_i(0)) -- is it
obtainable with some change of variables formula, like multiplying pT by
the jacobian of the map from x(T) to x(0)?

basically we have x(0) = f(x(T)), where f contains the whole ODE solver
used to follow the characteristic curves, which is differentiable.

Turns out this is the good old multivariable change of variables formula.
Basically we scale the input pdf by the |(df(x)/dx)^-1|. BUT for this the
change of variables needs to be bijective, this is not given if we have
multiple locally optimal trajectories, as is usual in many interesting
cases. what happens then???

If we ditch the jacobian and assume it is 'constant enough' we can lump it
with the other scaling factors on the t=0 side. then we get more of a
heuristic method of the type "sample terminal conditions, throw away ones
we don't like, sample more of the ones we like". Which might still be
great, who knows \o/ After all the jacobian is not that informative, it
will be very large anyway, dominated by the fastest dynamics, this is
exactly the well known sensitivity thing.




---- other ways to possibly think about this problem ----

normalising flows???
cross entropy methods in optimisation (probably pretty close to above)
brute force: sample lots of x(0) and find initial costates with usual gradient methods


2023-05-10

meeting this morning. talked about new sampling-then-approximation path
which they generally like. GPs instead of NNs after all?

Spent all day trying to implement importance sampling (iteratively, with
growing horizon, so we don't have so wildly different distributions). Am
mainly fighting with diffrax's weird way of handling time steps.

write a simple own rk4 loop with fixed width????


2023-05-15

smoothed out a couple kinks in the importance sampling code. ditched
writing my own ODE integrator and wrote code that fits the design choices
of diffrax instead (thanks @Patrick Kidger!). Fixed a bug where invisible
shape-rank-promotion-broadcasting stuff was happening. it works quite well
now. also made the plotting nicer.

BUT remark: several things should still be considered.
- if we increase the time horizon, as we will have to if we want to model
  complex multiscale systems, the sensitivity problem pops up again, as we
  cannot reliably represent gaussians with very small covariance matrices.

- there are several approximations at play.
  - The distribution is 're-parameterised' as a gaussian at each iteration,
    removing the possibility of modelling multimodal distributions, which
    might well arise in practical settings (-> nonconvex decisions,
    intersecting characteristics).
  - we ditch the factor of det(jacobian_x(f(x))), where f is the function
    mapping terminal conditions to initial conditions. this means we don't
    have the actual sampling density in the importance sampling likelihood
    ratio, only something roughly proportional to it. in the (very small)
    example it works well nevertheless.

so if we want to improve it even more, basically two possibilities exist:
- make the sampling much better by modelling a wider class of distributions
  than gaussians (gaussian mixture? particle filter style?)
- go with this approach, later generate more data based on some uncertainty
  estimate
  - very natural with GPs, since they give us a) predictive uncertainty and
    b) the weights of how much each training example contributed to the
    prediction, so we basically have a nice 'approximate inverse' of the f
    mentioned before and can just say that if x0 = w.T x0_train, then
    probably also the same convex combination can be made in terms of the
    terminal conditions, together with some local linearity approximation
    at least.


TODO tomorrow:
- read up on GPs
  - jax implementations
  - conditioning on gradient observations
  - hyperparam/kernel optimisation?

- think about ways to approach stability guarantee
  - probably: bounded approximation error -> stability?
    - probably need some dubious assumptions on smoothness etc.
  - hoeffding like hertneck et al 2018? bit of a cheap way out



2023-05-16

first removed the weird (..., 1) shapes from all the code. also refactored
a bit generally. took quite long.

basic idea on a high level:
- leave sampling code as it is, it does not need to be perfect
- instead rely on GP uncertainty estimation to generate more samples as needed

so the whole algorithm could look like this:
- find initial data set of optimal trajectories using current sampling code.
- approximate (V/V_x/u) with GP (or BNN, NN ensemble)
- while (approximation not good enough):
    - Find point x with high uncertainty estimate
    - Our GP approximation looks like: E[u(x)] = Σ_i k(x, x_i) * u_i,
      where x_i and u_i are the known 'training data'.
    - We form a linear combination of terminal boundary conditions, with
      the same weights k(x_0, x_i), and hope it will give interesting new
      training data through the PMP, at some state close to x.
    - re-fit GP/etc.

thoughts on GPs:
https://github.com/JaxGaussianProcesses/GPJax
https://github.com/JaxGaussianProcesses/MOGPJax
https://proceedings.neurips.cc/paper_files/paper/2018/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf

either we use a fancy gradient-enabled GP to condition on value function
and its gradients. OR we just fit the gradients using more usual gp tools.
or even just directly the controls? we have all the data anyways... maybe
interesting to compare these options with each other...



anyways, thoughts about the software architecture side of things:

we would profit from having a sampling thing that is jitted once and can
subsequently run quickly. maybe a 'sampler' class, with methods for
a) finding suitable terminal condition distribution (what it does now)
b) only doing batched backward integration of PMP (to refine solution)

we need a GP implementation from which we can easily get the weights/kernel
matrix. probably all of them do this, should be simple. Also need a decent
way to store variable-sized data without destroying all the nice jax stuff.


2023-05-17

cool blog posts about conditioning a GP on derivative observations. seems
to be not too hard, basically if the observation is a gradient we swap the
kernel function for the gradient of the kernel function? can it really be
that simple? although it seems we make the kernel matrix much larger,
because the derivative observation is nx-dimensional, so definitely not
really scalable... but nice testbed probably.

interesting experiment would be to compare different methods of GP
regression, for example:

- fit V(x) using only value observations, compute control by argmin_u H
- fit V(x) using V & V_x = λ observations, compute control by argmin_u H
- fit λ(x) directly, compute control by argmin_u H
- compute controls by argmin_u H, fit u(x) using GP

criteria to optimise could be the test error on a holdout set, or the
closed loop control cost. also choose whether to do this with the same
dataset for each method or including some sort of uncertainty sampling.

https://gaussianprocess.org/gpml/chapters/RW.pdf, p191: 9.4 derivative observations

http://herbsusmann.com/2020/07/06/gaussian-process-derivatives/
http://herbsusmann.com/2020/12/01/conditioning-on-gaussian-process-derivative-observations/

https://blogs.ed.ac.uk/datasciencecdt/2020/08/31/derivatives-and-gaussian-processes/
https://github.com/sighellan/GP_derivatives_example/blob/master/Derivative_example.ipynb

https://docs.gpytorch.ai/en/latest/examples/08_Advanced_Usage/Simple_GP_Regression_Derivative_Information_1d.html


https://wessel.ai/stheno/docs/_build/html/readme.html

did some small experiments. basic gaussian process example, 1d, toy
function. just like in all the textbooks :)


2023-05-17

task for today: decide how to implement gp with derivative information.

a) use GP library ideally with that functionality? haven't seen one. can
   we hack our way into that functionality? basically, assign different
   kernels to different observations.

b) do it from the ground up. a very simplistic GP toolbox is basically a
   couple of manipulations on large gaussian distributions. then we could
   easily populate the K matrix with the right (value or gradient) info.


2023-05-23

finally implemented GP with derivative observation. used tinygp, a
(relatively) low level GP library, but it makes it easy to define new
kernel functions etc. in retrospect it was not too hard, just as always it
took some time to figure out which data to put where in what shape.

next steps, probably: wrap it in a class that provides high-level
operations without much boilerplate, try to use that for the 'complete'
algorithm of iteratively refining the value estimate.



2023-05-26

starting to combine the basic ingredients. idea:
- generate initial trajectory samples with pontryagin sampler
- fit GP
- iteratively sample more trajectories where uncertainty is high.

intuitively I was just going to take the uncertainty in V(x) to take more
samples. but for the controls ultimately, grad_x V(x) is relevant. granted
when we have samples of V(x) and V_x(x) at the same locations it probably
won't matter too much. but still, we should think about how to adaptively
sample if we decide to adaptively sample.

the other approach would be to use a better sampling method in the first
place which guarantees that we actually reach some desired distribution.
but that is probably a bit harder...

also tried to implement a simple approach where after finding some new
states with uncertain V estimate, we try to 'predict' which terminal
conditions can generate new data there, by basically reusing the GP that
predicts V from x(0) to predict λ(T) from x(0).



2023-05-29

tried a bit more to do the last thing, turns out it is not so easy, if I
manually apply the gaussian conditioning formula with (I think) the right
kernel matrices inside, I still get different predictions than with the GP.
In any case, a method like this would not scale to NNs at all. Maybe it
would be easier to do something like this:

sample trajectories, fit V

loop:
  sample new trajectories
  subloop:
    adjust the terminal conditions in direction of ascending uncertainty.
  add these new trajectories to the training set


this could easily be done with GPs and NNs, probably by connecting it to
the right theory in the realm of sampling algorithms etc. we can show that
it is an approximation of sampling from a distribution proportional to
exp(uncertainty estimate), a boltzmann-ish distribution.

problems could be the sensitivity problem again - probably difficult to
tune gradient descent algorithms. maybe if we do some PCA-like
transformation at the input?

or ditch gradient altogether and do everything as a sampling-type
algorithm? as in, sample new trajectories and then run a evolutionary
algorithm kind of thing to arrive at a set of terminal conditions with high
value estimate uncertainty...

generally I see a couple different flavours of this kind of stuff:

1. separately maximise uncertainty in terms of x(0) directly, and then
   search for terminal conditions that give information there.

2. maximise uncertainty as a function of x(T). This would be the nicest,
   but maybe leads to sensitivity problems etc. but with the right tricks
   it might work. i am thinking of
   a) reparameterising the input space by pre-scaling Σ^(1/2), the sqrtm
   covariance of the terminal condition distribution found by the
   pontryagin sampling code
   b) trust region.

3. maximise uncertainty in terms of x(0) as well, but after each gradient
   step, estimate a new terminal condition (probably w linearization), and
   simulate from there to get next 'actual' x(0).

probably 3 is most promising? at least in terms of immediate practicality.
on the other hand I feel like 2 is the one which would lead to the best
scalability.

decision for now: investigate 2. seems the nicest in terms of theory,
probably more computation than strictly necessary, but it is offline so who
cares \o/ still better than RL hehe


2023-05-30

did some small steps, first implementation of the complete algorithm is
running, very small example and GP. it goes like this:

generate initial set of PMP-optimal trajectories with importance sampling
fit GP
loop:
    generate new terminal conditions from same distribution used initially
    take N gradient steps adjusting x(T)/λ(T) in direction of ascending value uncertainty
    re-fit GP with that data

probably this can nicely be explained theoretically as a sampling algorithm
(MCMC/SGLD style) that samples from a distribution concentrated around
regions (in terminal condition space) that lead to uncertain value function
estimate in x(0) space. this in turn means we should be able to 'cover' the
complete interesting state space at t=0, because, for example:

- the interesting region is contained within the set for which optimal
  trajectories towards the goal exist
- the sampling algorithm (in theory) can explore ALL of x(T)/λ(T), so for
  every interesting initial condition there will be a good value estimate
  eventually.

or, ditching the first assumption, then we get at each initial state x(0)
one of two outcomes:
- either the sampling algorithm eventually finds terminal conditions that
  inform the value function at x(0), to some desired accuracy
- or there exists no such terminal condition, which will lead to large
  uncertainty at x(0), so with a sublevel set of the model uncertainty we
  get an estimate of the stabilizable set as well :)

this looks actually promising, many practical things can be addressed
nicely like this. for example, the interesting region can be added as a
penalty term which decreases likelihood significantly, making the sampling
algorithm naturally 'uninterested' in regions outside the pre-defined
interesting region, which is not given in the current mockup
implementation.

furthermore, (i think that) nonconvexities/multiple locally optimal
solutions could be handled gracefully. by defining some desired
distribution over the state space at t=0, we also define a distribution
over x(T)/λ(T), but not in the obvious sense - if there exist different
local solutions starting at some x(0) then the corresponding probability
density will show up twice in the terminal condition space. this causes
issues with normalization, probably one of them will have to stop being a
proper probability density function, just some sort of handwavy analogue,
but that should not stop us from trying to sample from it. but probably not
in this semester project...

also, the distributions can also be quite ugly for this type of algorithm,
which is exactly what will happen once we try to include state constraints,
which will introduce additional costate-like lagrange multipliers which
probably will need to be found at T to produce optimal controls at some
state at t=0. but surely not in this semester project :o


(update in the afternoon)

changed the algorithm to include state constraints during sampling (not in
the OCP just yet...). Basically, we define a 'desirability function', equal
to uncertainty - constraint penalty, so the desirability becomes very
negative shortly after leaving the interesting region. the desirability
depends on x0, but desirability(integrate_PMP(terminal_condition)) can be
composed trivially to get it as a function of x(T)/λ(T). If we successfully
find all local minima of the desirability in terms of terminal condition,
we have great data to further inform our function approximation. this can
be done in principle using GD+random initialization or some fancier MCMC
sampling technique.

does this also address the (potential) sensitivity issue when using state
constraints in the form of penalty functions? then there will be small
regions in terminal condition space which correspond to large regions in
x(0), at which we will need many samples to 'cover' said large x(0)
regions... in principle this might work by just locally optimising
desirability(λ(T)) many times, or by sampling from p(λ) ~
exp(desirability(λ)).

but to actually achieve a distribution over x(0) in the latter case
we will need to be more proper - by incorporating the change of variables
formula for density, with 1/det(jacobian(f)) where f maps from term. cond. to
x(0). even then we are not guaranteed to get a probability distribution
back, because f is not guaranteed to be a bijection, so the jacobian rule
does not even really apply... but maybe we still get something we can treat
as a (negative) logpdf for fancy sampling algorithms.

todo next:
- smooth out implementation kinks, see if it really works
- build a half decent example, with performance metrics/closed loop control
  plots/bells and whistles.
- simultaneously:
  - start writing, formalising the concepts
  - try to apply it to a slightly more interesting example
  - decide whether or not to also do the more practical, NN-ensemble type
    approach.



2023-05-05

been thinking about the main algo a couple of times. basically, I think
both of these approaches should be similarly good:
- search for terminal conditions that are 'good' directly
- search for initial states that are interesting, then find corresponding
  terminal conditions.
If we already have a large 'library' of solutions, 2. should also be easy
by applying gradient based search to some number of closest neighbours to
get a new solution. maybe this is even the more practical approach? but
probably for simple problems it won't really matter, and I think 1. is
simpler in terms of implementation and theory, so lets stick with that.

Is my intuition wrong about pre-normalising data to 'help' gradient
descent? suppose for an abstract problem, we are trying to find
argmin_x f(x), where f contains both the PMP solver and the desirability
function. We have two options:

a) directly use gradient descent in terms of x to minimise f(x)
b) gradient descent in terms of z, x=Az, to minimise f(Az)

What do the two gradients look like? For a, obviously we have some gradient
grad_f(x) which AD will calculate. The gradient update then is:

x+ = x - η grad_f(x)

For b, we have: d/dz f(Az) = df/dx dx/dz = grad_f(Az) * A, and the
gradient update in terms of z is:
z+ = z - η grad_f(Az) * A

converting the z to an x by left-multiplying with A:

z+ = z - η grad_f(Az) * A
x+ = x - A η grad_f(x) * A

does this even make sense? why are there two A's? this smells wrong. should
one be an A.T?

so basically, the gradient dynamics do NOT stay the same and we may or may
not get improvements by the pre-scaling.



anyways, made some practical progress. implemented an unadjusted langevin
sampler (basically, gradient ascent on the logpdf with some noise,
practically the noise size ends up being the tuning parameter alongside dt
which is the step size). Really nothing special, but seems to work alright
on the simple 2d toy problem.

HOWEVER: made a plot of desirability function in terms of (normalised)
terminal condition, and it is appears that the geometry is quite warped
even for this simple system. specifically, large regions in x(0) space are
mapped to small regions in terminal condition space, basically with the
large swathes of x(0) which cause higher control cost mapped to small
pointy tips in terminal condition space. this is generally said to be
challenging for MCMC sampling algorithms... what do we do :o  maybe the
'first generate lots of data then find out how to generate more suitable
data with batched local optimisation' approach is smarter after all? dunno

maybe this could be alleviated by incorporating a 'resampling' step in the
sampler? this would definitely depart from the MCMC sampling realm into the
heuristic global optimisation/evolutionary algo realm, but it might work.
idea: every n steps, resample the particles according to their current log
pdf or similar.

other idea: actually do the jacobian correction, bc. it seems that the
factor by which areas are squished (= det of jacobian) is not at all
constant over the state/terminal condition space. maybe that would
encourage sampling algorithms to give more weight to the small regions at T
that affect large regions at 0. well, not maybe, definitely. depends on
whether it is practical but honestly this should not be too hard.

or, other idea, maybe there is some smart homotopy thing? like the initial
importance sampling sampler, or the 'time marching' method from
nakamura-zimmerer et al., that kind of makes the nonlinearity easier.

somehow the code makes a segfault when mcmc_plot = True.


2023-05-06

maybe it gets better when we replace the ULA sampling with adam, helped by
some 'exploration noise'? lets do it.

am doing it. basically, single adam optimisation with scan wrapped in a
function, then vmap. maybe it works better? -> a bit, but geometry still
challenging.




yet another idea: we still follow the paradigm of sampling in the terminal
condition domain to get optimal trajectories from interesting initial
states, but we somehow inform the sampling procedure with the jacobian of
the map from tc to x0? that might alleviate the large squishing in state
space. like, an evolutionary strategy or noisy gradient thing or whatever,
but in a space linearly transformed to look like the x0 space, instead of
the tc space which can have weird geometry.

actually I think this idea has great potential. it would address the
problem of weird geometry, using information we basically have at small
cost (the jacobian of the λ(T) -> x(0) map), might help with exploration
and maybe also bridge a theory gap between distributions in the two spaces?
maybe the last one is a stretch. but practically we might get away with
just acting like we're doing ULA/SGLD/whatever in x(0), nice and easy,

this is also reeeally close to the idea I had before the project (do SGLD
on x(0), keep some local optimal control prediction, update it easily with
a couple newton steps or similar). the only difference is basically we do
SGLD in the T space but adjust it to approximate the 0 space, not the other
way around. okay maybe that is somewhere between handwavy and plain wrong.
i really should eat something right now...

okay, ate something, have energy again.

maybe if we do this (proposal distribution scaled by jacobian something) we
can also completely get rid of the initial pontryagin sampler??? instead we
might be able to just start at 0 (if 0 is an equilibrium) and then explore
the state space efficiently because the method would be an approximate MCMC
on an easy distribution over x(0).

could the same idea also be applied to the deterministic analogue (multiple
initialisation, adaptive gradient descent to find local maxima of the
desirability function?) this would basically amount to pre-conditioning the
gradient somehow with the jacobian...


am still on it, feels like it is almost working, but some weird bug is
here. jax.grad(desirability_fct_x0)(np.array([0., 0.])) gives nan, but for
values other than 0 it works. whyyyyyyyyyyy? same if using jacobian. feels
like there is some pole-zero cancellation somewhere inside the funciton.

fixed that. was a NaN due to differentiating a sqrt at 0, in
desirability_fct_x0.

spent some time running and tweaking the sampler. I cannot say that it
really works. sometimes there are still unexplainable NaNs, sometimes we
have an acceptance probability of 0 and get stuck, dunno too much. but I
feel this should work quite decently, there is reason to develop this
further regardless.


2023-06-07

the sampler works now! key was to clip the desired jump size in x0 space.
this pervents situations where we go to really large numbers for seemingly
no reason.

thought: several options to accelerate mixing at the start during burn-in:

- have a higher dt
- have a larger noise value
- increase the 'temperature', i.e. sample not from logpdf but from a *
  logpdf with a<1.

are these all the same? what are the differences?

the central update is:
x+ = x + dt grad_x(log p(x)) + sqrt(2 dt) noise


2023-06-12

let my sampler run for a LOT of iterations with basically just a 'truncated
gaussian' target distribution over state space (x0). am quite sad because
it seems like it reaches quite a different stationary distribution...

ways to try to fix this:

a) try to achieve the exact proposed step in x(0), by finding the (for
practical pruposes) exact λ(T) that gives info there, probably with a
couple of newton steps this is doable.

-> would be doable but kind of inelegant, and it involves generating even
more trajectories which we throw away again, so maybe not the most
efficient.

b) reason about the proposal distribution, come up with a correct M-H
accept/reject step that actually achieves our goals.

-> would be the cleanest, but maybe not easy theoretically, and also maybe
the resulting proposal and M-H step (especially evaluating the forward and
backward transition densities) would involve extra computations all the
same.

c) don't try to fix it, rely on iterative uncertainty sampling with
x0 ~ log(GP posterior variance - state penalty)
to find data in 'all' state space regions.

-> probably most practical, I've seen the sampler work quite well for this
type of distribution, but then we again lack guarantees bc. we have a
biased stationary distribution, all we can do is say "look it kind of
works".


In the spirit of a) I ran the sampler with 1, 2, and 3 newton steps to try
to achieve the correct sample. but somehow it didnt work, the distribution
is still really skewed w.r.t. the desired one. probably this is a shit
idea.

BUT i have a new idea.

can we adjust the proposal distribution across state space? probably yes,
because we are ensuring the detailed balance condition if the M-H
acceptance step is done properly, with any transition density.

is this usually called 'adaptive' or 'nonstationary' mcmc? research more

so, probably we can do it more elegantly by directly operating over λT, but
still incorporating the geometry information (jacobian of PMP function) to
propose steps that result in somewhat sensible steps *in state space*. Then
by doing the M-H accept/reject step all in the λ(T) domain we ensure that
despite the weirdly chosen proposal, the stationary distribution is as we
want. only thing to watch out for is to apply the correct
change-of-variable formula to evaluate the desired density in terms of λT.


just to summarise and improve understanding again, this is what it would
look like. We want x ~ 1/Z exp(desirability(PMP(λT))).

define a proposal density that results in 'sensible' state space jumps.
this is basically the same that we are already doing, but instead of just
calculating the realization of the random variable we also need the
density. that should not be too hard. so lets do it.

alright, I did this. jit worked, am waiting for results. more details in
report.










also, noticed that previously I had an error in the calculation of H (the
acceptance probability), basically I inverted the fraction between the
forward and backward transition densities!!! Now it looks much better.


2023-06-13

debugging the new stuff. I think it should work but it doesnt really. it
oscillates between two clusters of points forever. tried changing it to
random walk M-H (without langevin/gradient stuff) and still it looks sus.
but at least it does something.

now it works nicely. important bugfixes in commit: 27c91202. ofc it was
some really basic error. now it works very nicely actually, both sampling
from a uniform distribution over the relevant state space subset, and
uncertainty sampling look flawless, and it works also without the hacky
clipping of the step.

also, I need to decide on terminology. in the paper I used some new terms
which I feel are nicer:

'desirability function' -> reward function r(x)
'integrate, PMP integrator, etc' -> PMP: λT -> x0
dPMP(λ)/dλ -> J  (just for shortness)

so should i adapt the code as well? would be nice right









more ideas:
- try to sample from a sublevel set of the value function V(PMP(λT)), so we
  know that in closed loop we won't push the state outside of the region
  where we know the controls.
- simulated annealing type thing in the sampler, to (maybe) improve
  mixing/global optimisation capabilities, where we sample ~ exp(-T f(x)),
  and sweep T from 0 to 1. Or only the T associated with the actual
  desirability/reward, not the state penalty.


2023-06-14

meeting w/ lenart & bhavya.

sampling algorithm works nicely, i present, they ask a couple of questions.
bhavya asks why we don't just do uncertainty sampling in x0 and then find
the relevant λT. good question - the whole project was built on the premise
of avoiding the NLPs associated with this. maybe write the motivation for
this kind of approach more clearly in the project.

other than that, they say whats needed currently is a more thorough
writeup, specifically with clearer assumptions and problem setting, and
with a concise overview of the whole algorithm. then we can think about
experiments and theoretical underpinnings.

they mention that publication would be a real possibility and would
encourage me to do it so long as i want. discussion on where to try and
with what time postponed.
