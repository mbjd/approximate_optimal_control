2024-01-15

decided to use this log again for daily stuff.

looked into value expansion version again, from DDP type solvers in continuous
time. https://dcsl.gatech.edu/papers/acc15e.pdf here is a nice derivation for
the HJI equation.

also debating: if we go with this value expansion option, we will have to do
more computation per trajectories, the state is enlarged by an (nx, nx) matrix
(symmetric so maybe half that). is it worth it? can we effectively explore the
state space without too much unnecessary data?

thinking a lot about the "fundamental limits" of this purely backward-in-time
way of solving the problem. inevitably the bottleneck will be travelling along
long, turnpike-type trajectories, when the fast dynamics, which are unstable in
backward time, constantly push us away from it. Will we actually end up
creating much more data than what would be needed? Would it be a smart(ish)
idea to throw away *some* of the trajectories to limit dataset size?

also, inuitively, it seems like higher dimensional problems are even harder
from this perspective. If we want to re-start HJB characteristics away from the
terminal set, we need accurate knowledge of λ(x) and V(x) at that point. This
in turn we can only do when the data density in that region is alredy high, so
that either some nearest neighbor interpolation or taylor extrapolation is
accurate enough. But does this even work in high dimensions? Basic "curse of
dimensionality" intuition says that as dimension increases, this type of
nearest neighbor search becomes increasingly a bad idea: The dataset size
needed to satisfy some fixed data density is exponential in dimensionality.

(another interesting question here would be to challenge the assumption that we
need accurate V and λ to continue a HJB characteristic curve. If λ is slightly
wrong, can we prove something about the suboptimality of the "stitched"
characteristic curve w.r.t. the actual optimal trajectory?)

In the same vein, my initial assumption kind of has been: Better to "stitch
together" optimal trajectories by re-initialising the characteristic ODE (a),
than to re-calculate the whole final part of the trajectory (b). Is this true
tough? To do (a), we have to interpolate the costate and value info, which in
higher dimensions might be problematic. (and even if the sets from which most
backward trajectories start locally resemble a lower-dimensional subspace,
interpolation requires lots of care to not go "outside" of that subspace too
much. This is also something I think where quadratic value expansions can help
a lot: the hessian of the value gives us a nice distance metric, which maybe
encapsulates this "lower dimensional turnpike subspace" thing which hopefully
exists)

all of this makes me ask the question: would a DDP type solver be a better
overall fit? the benefit of our approach (hopefully) is that we only generate
(locally) optimal trajectories, wasting no time on iteratively adjusting
suboptimal ones. However, that advantage can be nullified if in the process of
exploring the state space with optimal trajectories, we generate much more of
them than what we would need for training an NN. Maybe we could have made a
similarly good dataset with much fewer DDP-generated optimal trajectories, even
if they are a bit more expensive to calculate, but we only generate the amount
actually needed, which probably is a lot less than the current idea.

this makes me heavily question the purpose of the project. if we just take a
DDP solver, why not one of the 100s that already exist? If we do our
characteristics type thing we can already guess the conclusion*, so why do it?

* something like:

we attempt to solve control problems to global optimality by evaluating optimal
trajectories backwards. end up with no global optimality guarantee, best case
some probabilistic asymptotic approximate thing. It works great for
"non-multiscale", low-ish dimensional problems, for others we end up generating
huge datasets for not much tangible benefit, all while algo complexity is
basically O(dataset size^2).

ways to address this:
- be aware that it probably won't be state of the art, have fun exploring
  algo details and heuristics to speed it up and stuff.
- connect with previously existing methods (ddp) and work from there to some
  sort of global optimality guarantee
- do a 180 and start a somewhat different project? (PINN? active exploration?)

also still kind of debating thoughts from last week: should we go all in on
making it efficient with the right data structures? store trajectories in a
tree or DAG like thing based on "flow" of information? use some black magic to
speed up nearest neighbor search, like k-d tree or locality sensitive hashing?
because pretty much no matter how we go about it, we will not only create a
sizable data set of trajectories, but also have to do many, many
nearest-neighbor-type searches over it.

this was a lot of maybe incoherent rambling. sorry to my future self and
whoever reads this. maybe the final paper will be more readable \o/



2024-01-16

looked through literature. found out that our gradient enabled neural network
is already known under one more name: Sobolev training (sobolev spaces are
function spaces endowed with a norm that looks like a p-norm of a vector of
function norms of all partial derivatives up to order k). specifically there is
also an interesting phd thesis:
https://gepettoweb.laas.fr/articles/amit_icra_22.html. They basically use a DDP
type solver coupled with global value function approximation, to find infinite
horizon optimal control over a large domain by iteratively using the learned
value function as terminal cost.

Should we try to expand this to some sort of provable global optimality?
Because I don't think they do that there. There the claim "So, when used as a
proxy for terminal cost functional, ∂PVPv [name of their algo] tends to drive
the locally optimal solver toward the globally optimal solution.", but no proof
or explanation.

possible decent contribution would also be to do the infinite horizon in one
single trajectory with some sort of time reparameterisation. reduces the number
of approximations and weak links, and I think with adaptive solver this can
actually be quite efficient. In fact, I think inf-horizon DDP itself would already
be a cool contribution, even without the "hopefully global" optimality.

other new interesting papers in semanticscholar folders:
- ddp-ish value expansion
- local -> global optimality
- sobolev learning
- V approx, transfer learning, etc.


small tidbits i thought about:

- in continuous time DDP apparently the hessian Vxx is always positive
  definite, in contrast to the more often used time-discretised version. Does
  this hold even when we have a really sloppy numerical solver? would be cool if
  yes

- NN learning: only consider data points with v(x) <= v, and sweep up v during
  training? this way we might explicitly identify the points when different
  local solutions start conflicting, and maybe do something about it.


2024-01-17

had meeting. talked about all the concerns from last few days and got
reassuring answer that yes, these are hard and, in practical terms, unsolved
problems. advice: just explore around for another week and slowly try to come
up with a sensible research question.

mentioned frustration that "everything" has already been done, no "niche" left.
answer: "own" niche can also be very small, or a contribution to extend
existing work can also be quite small to be significant.

current options:

- explore (probably limited) possibilities with pure PMP backward integration.
  also sometimes labelled geodesic flow, geodesic spray, etc. probably only
  simple systems, low dim, not multiscale.

- explore backwards PMP integration + quadratic value expansion. the appeal is
  obviously still no iterative optimisation over trajectories/inputs. but
  scalability concern due to instability and huge dataset remains -- can we
  still optimise the hell out of it? with smart proposal strategies or fancy
  data structures?

- go for a DDP like solver method, similar to PhD thesis of Amit Parag, and try
  to come up with tricks to "encourage" global optimality. Lots of ideas around
  BNNs with asymmetric loss, Sobolev learning, active learning, etc. currently
  this does seem the most interesting to me.

- actually already infinite-horizon, continuous-time DDP could be a cool
  contribution itself...?

- sth else?

different, mostly-orthogonal directions to expand the problem setting beyond
inf-horizon, continuous time:

- some sort of approximate *global* optimality. probably though there just
  isn't an actual shortcut to this. give up?
- input constrained (in general dims -> QP solver finds u*)
- state constrained (in the sense of safety limits)
- state constrained (in the sense that x is on a manifold)
- robust (with adversary, HJI equation)
- parameterised family of dynamics


other than that:

- devoured more literature about continuous-time DDP-ish solvers, especially
  nteresting ones to do w/ HJI/robust stuff.

- found a nice paper comparing different BNN methods to the "true" posterior
  obtained with Hamiltonian Monte Carlo, put it in a new BNN folder.

- found a couple more papers about what I think is essentially the same as
  pontryagin backward integration like in SA, one example was planning min-time
  flight trajectories on the actual globe with known wind field and simplified
  single integrator dynamics. that is actually a cool application, where the
  multiscale limitations do not come into play.

Random assorted thoughts, about making DDP work in inf-horizon:

I am doubtful as to whether the time-value rescaling, or some different time
rescaling, is needed. Probably the adaptive solver will be just as efficient
without it [citation needed]. What is more interesting then is how do we choose
the time horizon. Simple first idea: just choose a large-ish time horizon and
define a terminal set Xf (eg. LQR value sublevel set). Then, optimise
trajectories using DDP. If the trajectory ends inside of Xf, good. If not,
increase the time horizon. In continuous time and w/ adaptive solvers, [i hope
that] a "too" long time horizon will not be much of an issue, especially if
most of the time is spent within Xf, where the ODE solver can choose large
steps.

And about distributions/ill conditioning/timescale separations:

also, many of the problems we see (ill conditioning, bad distributions) arise
from basically the fact that systems are decomposable across time scales.
Traditionally, this makes engineering solutions easier, however if we aim to
solve the whole problem at once it becomes much more difficult. Can we somehow
use these time-scale decomposition heuristics to maybe improve/inform our
sampling process, while still ultimately finding the actual solution to the
full problem? not so sure how exactly to go about this.

about possible approximate global optimality:

this seems admittedly hard and convoluted in the framework DDP + Sobolev BNN
value function. One possiblilty: keep all trajectories in a data set. For each
proposal, also find nearest neighbor trajectories, and for each one of them (*)
try to do a homotopy method, moving its initial condition to the proposal while
keeping local optimality w/ DDP.

*: To speed this up we can first do some k-means clustering of the trajectories
in terms of lambda(x0), or in terms of some sampled points along the
trajectory.  Probably we get away with at most producing k=2 clusters since
(probably?) the set of points where n+1 different locally optimal trajectories
are equally good has one dimension less than the same set for n different
trajectories. So we might choose the minimum nontrivial number 2...

Other possibility: somehow use a sobolev BNN with multimodal posterior. But
this will be worth nothing if we don't have the data. you know, I am starting
to think that global optimality is just actually not worth worrying about, and
instead we will have to keep relying on engineering intution about the specific
system to decide if we are happy with the local solution at hand.


2024-01-18

thinking about ways to do function approximation which maybe works well given
the structured nonsmoothness encountered in value functions.

What would happen with a standard (B)NN (and enough data of suitable
distribution covering several distinct local solutions) is that we just kind of
smoothly interpolate between different local solutions, making the precise
"decision boundary" rather muddy. What we would prefer is that there is a
separate NN for each adjacent region, and that the output of the whole model is
the minimum of the individual models. That way we benefit from smooth
extrapolation (which over small enough distances should work quite well with
appropriate sobolev loss) without interference due to neighboring, distinct
solutions.

How could we achieve this behaviour, even approximately? We cannot just blindly
have N values passed to some min(.) type last layer, because of this:

due to the global structure of the value function, there exists a curve γ(s)
with γ(0) arbitrarily close to the nonsmooth decision boundary from one side,
and γ(1) from the other side, but with the property that V is smooth at all
γ(s) for 0 <= s <= 1. (intuitively, this curve can be constructed by following
one local trajectory forward to almost the goal/equilibrium, then smoothly
connect to the other solution, which we follow in reverse time until we reach
the opposite side of the decision boundary). Therefore, if we apply a naive
min(.) operator, we cannot exactly represent this because there has to be
another switching surface switching between the left and right regions,
somewhere along the curve, where V is actually smooth.

Therefore my next instinct is to approximate a min(.) operator with some
smoothed version thereof, and add an extra parameter which changes the
smoothness of it somehow. e.g. with some softmax thing. These are the
requirements:

- the soft minimum selection should be a close approximation of the actual
  minimum close to the decision boundary.
- the soft minimum selection should "fade out", i.e. change its selection only
  very smootly, when we are far from the decision boundary.
- there should be no deterioration in performance if we choose 10 different
  "classes" but only 1 or 2 are necessary.
- in general it should not completely break NN fitting.


in other news:

tried to understand some of the results on "PPO attains global optimality",
went over my head still. their convergence rate depends on the discount factor
though with a singularity at 1 so I doubt it is in any way applicaple to the
(IMO much more interesting) undiscounted case.


can we make ddp state constrained in an easy-ish way? I think I wrote this on
some note yesterday. basic idea: during the backward pass have two different
value functions: one from the standard unconstrained problem and one for the
constraint violation \int max(0, g(x(t))) dt. Then, during the forward pass, if
the constraint violation is nonzero, apply inputs minimising its local
expansion, basically replacing V with V_g, otherwise apply inputs minimising
the usual local value function. Does this work? If so, also in continuous time?
Are there problems due to nonsmoothness on constrained arcs?

is this basically a cheap, not-thought-thorugh version of an augmented
lagrangian type method? still want to grok those papers from Lutter et al. If
working that would probably properly get rid of the nonsmoothness problems.


2024-01-22

Yet another flavor of the main idea. For some time now I've been thinking that
while timescale separation introduces mostly problems for "full problem"
optimal control solvers (ill conditioning, uneven trajectory distributions,
stiff-ish ODEs), in the classical engineering setting it is mostly a blessing
because we can often design controllers separately over timescales (sacrificing
optimality but making it much simpler). From this angle I thought it would be
cool if we can somehow use this timsecale separation intuition to our benefit,
i.e. to make learning control laws more efficient, while still solving the
"full" non-decoupled problem.

Wrote a page in the overleaf idea dump about it ("Slow(er) Manifold following).
The idea is this: Instead of the RRT-style framework I thought about for some
time, try to find a backward optimal trajectory on the slow manifold (i.e.
where fast dynamics don't move much). This can probably be done with local
quadratic value expansion (= DDP backward pass) and by somehow identifying the
slow manifold or its local linearisation. Then once our trajectory inevitably
strays away from the slow manifold we can go forward in time along it until it
is ε-close to the slow manifold, then re-seed the trajectory "exactly" on the
manifold. The crucial change then is that we can immediately re-seed the same
trajectory, instead of doing nearest neighbor searches all the time. Then we
can save only the stitched together trajectory which actually stays (almost) on
the slow manifold and only do NN searches among this smaller dataset. Basically
we then reduce our sampling problem to a lower-dimensional one over the slow
manifold, where we first find "all" relevant causal dependencies and only after
that branch out in the fast subspace (maybe this will not even be necessary due
to already generated data as a byproduct). More thoughts in dump. Maybe this
would be cool still...

Other flavor: Modify the Hamiltonian dynamics of the PMP in such a way that the
slow manifold is stable in backward time, instead of unstable. However this
probably requires knowing the stable manifold in advance, which we don't in
general. Very hard to ensure that the trajectories of the modified dynamics are
somehow a good approximation of the unchanged dynamics with PERFECT
initialisation. This is probably a dead end tbh.

OTOH probably the full-blown inf-horizon DDP <-> BNN active learning loop would
probably be even more scalable and data efficient. But admittedly less flashy
and esotheric, and harder to pinpoint where exactly my "new" contribution will
be.

Both require quadratic value expansions so maybe I should start by working this
out properly in continuous time...

So the current selection, to summarise, is this:

a) Develop "inf-horizon" continouos time DDP, implement in jax. Connect with
   active learning & Sobolev NN to efficiently find new solutions over whole
   interesting region. Maybe some sort of "approximate" global optimality.

b) Try to take the "purely backwards" approach further, possibly like the notes
   just above


at the moment I am leaning towards a) which is probably the more generally
useful and scalable optinon. Main components of that:

- inf-horizon, continuous time DDP
  - theory (basically figure out those couple papers)
  - implementation :)

- Sobolev Bayesian NN
  - Which flavour of "bayesianisation"?
  - Sobolev how exactly? With hessian approxiamated by random Hvp?
  - Find some nice version that half works with nondifferentiable points?
    (see notes from Fr.)

- Intermingling of the two
  - How to propose new samples?
  - How to handle trajectory optimisation? Fixed iterations? Until converged?
  - Data structure considerations? Hopefully not
  - Reweight data, e.g. by 1/density, for training?
  - Bias somehow os we are more likely to encounter "surprising" trajectories
    which are better than the currently known solution?

- Other stuff
  - Which examples, costs, engineering tradeoff type goals?

this gon be fun

basic research questions to justify these goals (is this the valid way of doing
science? I am but a lowly engineer after all) :

- How can we achieve maximum sample efficiency? (by choosing correct NN and
  sampling strategy)
- Are we able to meaningfully speed up long-horizon trajectory optimisation by
  going to continuous time?
- To what degree can we automate the process and make it "hyperparameter-free"?
  (probably not that much tbh, I feel like optimal control is in the end just
  as much black magic as RL, perhaps with two or three less black first steps.)


other small stuff:

- had an idea over the weekend that H(x, λ)=0 describes something useful and
  could help in interpolating the right costate (in RRT idea). convinced myself
  though that this is nothing. for all other terminal value functions, we also
  have optimal trajectories with H=0. So this does not give much information.

- thought about other ways of modifying the backwards PMP system to make
  trajectories more well behaved. I think continuously modifying it, while
  always adjusting the V/Vx info using the local expansion, is a bad idea though,
  can't think of a way to ensure low approximation error.


2024-01-23

some more thoughts on how to represent the "multiple branched" value function
with an NN. got not much further than last time though. Current idea: have an
NN model which outputs k "possible" value functions Vi(x), and a smoothness
parameter κ(x), to finally give it to a "smooth min" type function to form the
final estimate. Scoured literature for descriptions of the "set of x where
different equally good local solutions are globally optimal", found lots of
names, made a section in idea dump.

Coded up a small demo of trajectory splitting with linear V extrapolation (from
costate), the rest constant, for really tiny "jumps". Looks okay, though I am
unsure how to verify it more precisely. Anyway I am thinking this is not really
the way to go.

Also had thoughts about trying to implicitly represent (= learn) the lifted
manifold, i.e. the set of (x, λ) described by our geodesic spray. However I
think this does not fundamentally solve anything, the "complexity explosion"
with longer time horizon is still the same, finding all local value functions
given some x is still an nx-dimensional rootfinding problem, and we make no
progress towards efficient global optimality with pruning of suboptimal local
solutions. Really I think I have to ditch "proper" global optimality soon :(

However, could we somehow "aid" the whole thing with engineering intuition? I
was just thinking always that the 2D quad has one obviously "nonconvex"
decision, which way to go around to go to the upright equilibrium of the
attitude subsystem (or perhaps how many times to rotate if we consider
outlandish states). Can we somehow encourage the local solver to "check" both
these cases? Maybe we can make something that obviously cannot in general find
global optima (we can always make pathological OCPs with arbitrary number of
almost equally good local optima: Think about a tiny UAV flying through some
sort of grid from a far distance). Maybe we can then adapt the NN to have
separate models depending on some simple condition, like: Is Vx saying we
should turn left or right? Or based on literally the number of turns from the
equilibrium. Although this could once more be some half assed solution that
only works in weird special cases...


2024-01-24

found another paper about continuous time DDP, which seems to have a nice
derivation of the propagation of the value hessian. todo read this more
carefully, and attempt to recreate?

https://dl.acm.org/doi/pdf/10.1145/3592454

also got convinced by bhavya that making some custom NN model suited to the
particular type of nondifferentiability encountered in value functions is
probably not the way to go. questionable results and not scalable to higher
dims (due to general lack of global solution).


2024-01-25

been reading that paper for some time. still don't understand all the
notational quirks exactly.

have gained new intuition about the riccatti eq. this is actually really cool:
instead of fiding the rapidly diverging backwards optimal trajectories adjacent
to our trajectory, we find a quadratic/linear parameterisation (= quadratic
taylor expansion of V(x)) of all of them. the good thing is that even if the
dynamics are really rapidly unstable, the associated taylor expansion stays
approximately constant, enabling us to "skip" all the local fast dynamics, and
skip ahead to finding out how to stabilise them locally.

From this viewpoint it is almost nonsensical to propagate Vxx information with
every trajectory in a purely backward PMP integration thing.


viable research goal:

learned *robust* optimal control a la https://arxiv.org/pdf/2107.04507.pdf? and
instead of a gradient GP we use a Sobolev NN for function approximation,
enabling us to use the second derivative. Going from there to optimise active
learning <-> TO interaction. Maybe heuristically "address" global optimality.

for robust control there are ugly technicalities with the terminal set, because
with an adversary we cannot stabilise all the way to the origin, because the
adversary has no input cost and always chooses the maximally destabilising
disturbance, whereas we do have an input cost and at some point stop caring
about small setpoint errors if we have to incur large input cost to fix it. A
very basic example of this is in Pachter & Weintraub.


~~~ other random idea ~~~

In https://gepettoweb.laas.fr/articles/amit_icra_22.html, they use a fixed time
horizon for prediction and use a learned value function as terminal cost. thus
the region where controls are accurately approximated is expanded gradually,
and they find infinite-horizon controls in the end.

I have been put off by this idea for some time because stitching trajectories
together gives us another source of approximation error (though I have the
intuition that this error is somehow very small), and also it gives us an
excuse to implement continuous time DDP, ofc with the hope that adaptive
solvers can "skip" the long boring "turnpike" parts of the trajectory quickly
and give effectively inf horizon optimal controls with feasible effort.

However, what if we do the following. We have a BNN with accurate uncertainty
quantification. To find the initial guess for TO, do a forward sim with
predicted u* from BNN. Evaluating uncertainty in u is no extra work so we might
as well do it, and stop the simulation once we are in a region where
uncertainty is below some small threshold. Then we do the TO with the BNN
(mean?) value function as terminal cost, over a much shorter horizon, hopefully
only concerning the fast part of the dynamics.

This *should* work intuitively right? If there is a "slow" manifold we expect
to have relatively many trajectories there because they all stabilise to that
region. Therefore we can hope there is a large region with low uncertainty
after some active learning iterations, and that this region is quickly reached
in the initial closed loop sim.

Are there concerns with the terminal state changing during TO, possibly to a
"worse" region? Intuitively, we should expect that it only changes to better
regions. Is this true? Try a proof:

Because the initial guess is a closed loop sim, it is a suboptimal trajectory.
Join it with the rest (from T to inf), suppose the rest is actually optimal
(reasonable since uncertainty low there). Then we know the cost-to-go from x0
will be lower after TO than before (bc. we've gone from suboptimal to optimal).
Does this tell us anything about the terminal state? Somehow I have the feeling
it doesn't if we have a time horizon in physical time, without warping...

We can also just check at the end whether the BNN at the new terminal state is
still certain enough, and if not, ditch the trajectory, or re-do with longer
horizon in the next iteration.




current list of very relevant other publications:

DDP treatments in continuous time:

Hutter et al.
https://dl.acm.org/doi/pdf/10.1145/3592454 (w/ general parameters for diff)
https://arxiv.org/pdf/2101.06067.pdf (w/ state constraints)


Sun et al, w/ terminal constraints
https://www.semanticscholar.org/paper/b00a864b457992c5f87a4e0e24382b33c26512f4
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10156422 (w/ control constraints)


+ HJI/robustness:
https://www.semanticscholar.org/paper/5ae21807233c95c5485da721a96cb9ff06a659de
https://arxiv.org/pdf/2107.04507.pdf

Trajectory Opt <-> Learning:
https://arxiv.org/pdf/2107.04507.pdf
https://gepettoweb.laas.fr/articles/amit_icra_22.html

CACTO: https://arxiv.org/pdf/2211.06625.pdf
CACTO-SL: https://arxiv.org/pdf/2312.10666.pdf (Sobolev!!!)

Initial Sobolev learning paper: https://arxiv.org/abs/1706.04859

+ something about BNNs/active learning but everyone knows how these work


...need to be precise about research question, specifically, what are we doing
...better than these?


2024-01-26


read the different continous time ddp papers for a bit longer. i think I gained
new intuition.

backwards propagation of V, Vx, Vxx is actually similar to directly evaluating
the characteristic trajectories backwards, but "modified" so they follow the
forward dynamics. Imagine the PMP characteristic is a particle following some
vector field (which it is), and imagine the Vxx term which we carry with it as
a local representation of the value level set, i.e.  the "front" of possible
adjacent trajectories. We can imagine replacing our point by an adjacent point
on the front, and if close enough, we should still get an accurate optimal
trajectory.

The DDP backward pass consists of essentially the same: propagating a particle
backwards in time together with the "front" of nearby possible other points.
The difference is the particle doesn't follow the vector field itself but
travels on the trajectory obtained in the forward pass, like on a rail. Then
there have to be some adjusting terms to account for the "sideways" movement of
our point. Because we implicitly (using Vxx) keep track of all nearby
trajectories, if the optimal trajectory is nearby (read: close enough so the
system is essentially linear in the region between), we have already found it
during the backward pass and just need the forward pass to "retrieve" it (and
to check whether it is actually optimal). If we are further away it is more
complicated, proving that we have a descent direction we can use for line
search is above my paygrade.

Now, the implementation is still not done, I still have to grok some notation
from the papers. Certainly this is the next task.

Also, installed ppopt (explicit multiparametric QP solver) and ran the test
example (control allocation on octacopter). It finishes in like 5 seconds and
gives a solution object with 500ish critical regions. But evaluating the
solution at some point does not work, it thinks the point is not in any
critical region. Want to find out why, also, am unsure whether using such a
poorly documented tool is worth the hassle. Maybe I just stick with 1D or 2D
input sets...



2024-01-30


copypasting from last time: the overarching project goals as of now.

a) Develop "inf-horizon" continouos time DDP, implement in jax. Connect with
   active learning & Sobolev NN to efficiently find new solutions over whole
   interesting region. Maybe some sort of "approximate" global optimality.

at the moment I am leaning towards a) which is probably the more generally
useful and scalable optinon. Main components of that:

- inf-horizon, continuous time DDP
  - theory (basically figure out those couple papers)
  - implementation :)

- Sobolev Bayesian NN
  - Which flavour of "bayesianisation"?
  - Sobolev how exactly? With hessian approxiamated by random Hvp?
  - Find some nice version that half works with nondifferentiable points?
    (see notes from Fr.)

- Intermingling of the two
  - IMO this is the actual interesting part.
  - How to propose new samples?
  - How to handle trajectory optimisation? Fixed iterations? Until converged?
  - Data structure considerations? Hopefully not
  - Reweight data, e.g. by 1/density, for training?
  - Bias somehow so we are more likely to encounter "surprising" trajectories
    which are better than the currently known solution?
  - some sort of heuristic global optimality thing? maybe assuming we know about
    the number or structure of the different "modes"?

- Other stuff
  - Which examples, costs, engineering tradeoff type goals?



been trying to grok the two main continuous time DDP papers again today. Tried
to develop my own intuition, connect w/ characteristics approach, and see
whether deriving the backward propagation from that angle gives us the same,
and maybe lets me understand the papers. Got a bit further on the theory front
but no implementation that is working yet.


2024-01-31


meeting. discussion about whether or not it is worth it to implement own,
continuous time optimiser. they say: probably not, probably they are right. i
can still do this if I get the rest working. then I can focus on the more
interesting part: how to do do active learning sensibly, how to handle
different local solutions, how to make it infinite horizon.

Problem is, in discrete time we get discretisation artefacts, we get no clearly
defined time invariant value function (or do we?) Also the inner optimisation
is now concerned with minimising the cost over an actual time interval, making
everything nonlinear and nonconvex and ugly. and yet more, we don't get to
profit from adaptive step sizes or anything like that. I really don't feel like
"throwing away" all the nice continuous time things. Is this a sunk cost
fallacy?

Stuff to look up:
- Offline RL (learning optimal policy from fixed, suboptimal data)
- average-reward MDP (undiscounted/inf-horizon problems)

goal for next time: formulate overarching goal, high level problem description
in cohesive and clear form


found yet another paper from Farshidian, Buchli, etc. about continuous time
constrained trajectory optimisation: https://arxiv.org/pdf/1701.08051v2.pdf
They state pretty clearly the advantages over NLP-based approaches:

"
    In general, NLP–based planning algorithms require the
    discretization of the inﬁnite dimensional, continuous optimiza-
    tion problem to a ﬁnite dimension NLP. This discretization is
    often carried out using heuristics, which can result in numer-
    ically poor or practically infeasible solutions. Our algorithm,
    by contrast, is a continuous–time method which uses variable
    step-size ODE solvers in its forward and backward passes.
    Given the desired accuracy, it can automatically discretize the
    problem using the error control mechanism of the variable
    step-size ODE solver. Informally speaking, this allows the
    solver to indirectly control the distance between the “nodes”
    in the feedforward and feedback trajectory. In practice, this
    decreases the runtime of an iteration, since the number of
    calculations decreases.
"


2024-02-01

finished own derivation of how the value taylor expansion evolves in the
backward pass. just couldn't help it. coded it up in skeleton form, still a few
things missing, I almost fear trying it out.


2024-02-02

continued hacking a bit at the implementation. been thinking about how we best
access the RHS used to make the forward sim in the backward pass.
mathematically we can just evaluate the time derivative of the solution
trajectory. however, it is a polynomial interpolation, not the real thing.
Compared the two with a plot and we indeed see rather large errors (more than
10% of the state values themselves). So, there seems to be no shortcut to
evaluating the RHS again, given that the ODE solver chooses different time
points anyways. And, come to think of it, we need the state derivative of the
RHS too which will dominate computational cost in all cases.

maybe using the interpolation is even better? the interpolation's time
derivative tells us where our actual approximate "solution" trajectory goes,
while the RHS evaluated again tells us where it "should have gone", loosely
speaking. will have to try it out. using the derivative is certainly easier
though. but even if we do that we need the previous input anyway, and to get
that reliably, the previous costate, and in turn to get that, either the full
data from the last backward pass, or some interpolation of the costate seen
during the forward pass.


ideas to start working next monday (high priority):

- formulate overarching goals neatly, maybe in own latex document?

- finish the backward pass implementation, debug, sanitycheck, work towards DDP
- if it completely fails, resort to trying to understand other papers

- think about active learning part and local/global heuristics.
- maybe start using some finished DDP implementation for other experiments?

low priority:

- implement inner convex optimisaiton in general case
  (brute force active set = region-free explicit mpQP?)

- come up with example dynamical systems illustrating different up/downsides of
  our approach

- think abt extensions (2 player differential game? parameterised?) in fact
  just the combination of those two (basically unifying Hutter et al. with
  Sun&Kleiber would be a great novelty but also considerable coding work)



2024-02-05

...vs what I actually did: re-do the calculation of \dot S along
characteristic curves. The thing from last time turned out to be not
symmetric, therefore it must be wrong (still unsure where exactly I went wrong
though). The second try is akin to the DOC derivation: write down a solution
of the characteristics plus some small variation as (x, \lambda) + (\delta x,
\delta \lambda). Then by subtracting the non-delta part and doing a first
order taylor approximation we get "differential dynamics", a time varying
linear system for the state variable (\delta x, \delta \lambda). like DOC we
then parameterise lambda as a linear function of x, S x. After juggling around
some terms the "differential dynamics" also tell us how this matrix S evolves,
we get something depending on different second partial derivatives of H^\star
and also S itself, looking very reasonable. Let's try it out in jax.


2024-02-06

was a bit sick in the morning, feared i had covid, slept in. was better by 10.

did some more philosophising about the differences between the DOC derivation
(LQ-isation of entire OCP about current iterate + solution of linear BVP) and
the weird thing I did (follow characteristic curves first and foremost, but
adjust the taylor expansion to account for the different direction in which
the forward trajectory points.) I may have found the central difference,
namely that in our version we are taylor expanding the pre-optimised
hamiltonian about the characteristic field given by current x and lambda, and
with it the u* map which is piecewise linear. In DOC this taylor expansion is
formed about the iterate trajectory. But also they have no input constraints
and no piecewise linear u*, just a linear one. Maybe read Sun&Kleiber more
precisely again to see how they handled it?

then, continued the implementaiton (of our, almost-characteristics-following
version) and got to the first run where numbers come out on the other side
juhuuu ;) On the first try the value and costate match the forward trajectory
(which was evaluated in backward time and so is already optimal). sanity check
passed!  The entries of the S matrix look kind of sus to be honest but then
again it is a trajectory which initially has the uav spinning quickly so that
is kind of to be expected. more sanity checking follows tomorrow, goodnight :)



2024-02-07

implemented the forward passs! it seems to work as expected!!!! except for some
small questions, such as: why does S = V_xx sometimes turn
non-positive-definite?


2024-02-07

had big enlightement this morning by feeling like the DOC derivation is the
correct one after all. by properly approximating the problem about the current
iterate with quadratic cost and linear constraints, then solving the resulting
linear subproblem, they basically build a standard newton method, with easy
proofs of convergence, line search etc. Went over the DOC style derivation
again, I think I have it correct now, first implementation looks alright
(symmetric \dot S and very close to other version). Not sure if they are both
actually the same. Maybe the only difference is really which linearisation for
the u* map we are using.

spent some time thinking about line search. in contrast to fininte-dim
optimisation, we cannot just add arbitrarily many iterates on top of each
other, but standard line search basically requires this. by letting x+ = x +
alpha * (descent direction) we are making x+ a linear combination of the first
iterate and all descent directions.

so we need to be somewhat smart about this. brute force solution:
- attempt full newton step, if some sensible descent condition holds, accept.
- if not, repeat backward pass, with an additional "regularization" term
  penalising ||x - x_iterate||. this gives still a LQ subproblem which we can
  solve exactly. Do the forward pass again, check descent condition. If OK,
  return; otherwise repeat the step with a larger "closeness" penalty.

Repeating the backward pass is expensive though. Other papers rely on just
applying a scaled down version of \delta x and \delta u. But with input
constraints this will be a bit complicated. Would much rather have a 'scaled
down' value function. But to do that properly we'd need to keep track of the
local value function that generated the last forward pass. And if that was
already made by combining the previous backward passes we need both of those
too.

easy heuristic which maybe works: Do only 1 backward pass. If descent condition
fails, then do another forward pass, but with local value function:

V(t, x) = v(xbar(t)) + λ(xbar(t)) (x - xbar(t)) + α/2 (x - xbar(t)).T S(t, xbar(t)) (x - xbar(t))

for some α > 1 which we may increase to increase the quadratic value term. this
corresponds to using a larger linear feedback gain which surely will not work
for significant α.

There must be something akin to "step in the descent direction but only a
fraction" that does not have all these issues...? how?



2024-02-07

did some further attempts at replicating DOC's S and s updates but to no avail.
mostly looked at pretty plots and figured out how to record webcam timelapses
from mjpeg streams.

points to start next week (this is essentially copy pasted from last week...)

- (still) formulate overarching goals neatly, maybe in own latex document?

- further debug & sanitycheck the backward pass, work towards DDP
- either decide on DOC or my implementation, or do both separately and compare
  (but stop switching every half day)

- think about active learning part and local/global heuristics.
- maybe start using some finished DDP implementation for other experiments?



2024-02-12

re-did the derivation from the chacteristics angle without the error I made
previously (in confusing a total and partial derivative of the hamiltonian).
Got the same fomula I have from the DOC style derivation for \dot S. looks
good.

did a couple sweeps (move initial state a bit, do a fixed amount of DDP
iterations) to obtain a family of (hopefully) optimal trajectories. Next up,
further sanity checking: Are the trajectories actually optimal? Find some way
to see how close to a solution we are.

But all in all this looks great. Was also happy about how quick it seems to be
after jit. it does 32 iterations, for the flatquad model in like 0.1 seconds :)



2024-02-13

continued playing around with ddp. made a cleaner implementation in functional
jax style with some extra abstraction. did some initial state sweeps. looks
pretty nice :) i think for the moment this should be workable.


2024-02-14

continued the implementation & some tests. made plots and brought meshcat
visualiser back from the dead. looks all pretty nice. also removed the
initialisation with backward shooting by a simpler one with LQR-controlled
solution near goal state.

2024-02-15

trying some more "difficult" states to sweep to. quickly the ODE integrator
steps explode. first remedy i am trying now: set maxsteps to something
outlandish like 10000, and set step controller dtmin to T/maxsteps.
theoretically this should work! but maybe at the expense of accuracy. there is
also the norm argument which might be useful especially in the backward pass:

norm: A function PyTree -> Scalar used in the error control. Precisely, step
sizes are chosen so that norm(error / (atol + rtol * y)) is approximately one.

in the forward pass as long as the system is about unity scale the default,
probably the 2-norm, should be good enough.

alright the problem is not the ODE integrator probably but some other error.
when sweeping from initial state 0 to Phi=pi/2, initially all goes well for low
angles, but suddenly, the angular rate goes either way up or way down. this
smells somehow like wrong treatment of input constraints to me. I'll plot the
input time series next.

they look quite decent, and already have significant constrained parts before
it goes wrong, so probably we are alright there. the backward pass in iteration
816 (first one that goes wrong) blows up to like 1e7 which the previous ones
don't do...

lowered solver tolerance on backward pass, and the particular instance of
blowing up is removed.

finding a new problem where NaNs creep up in sol.evaluate somehow. or wait, no,
it is the same old problem where backward pass blows up.

making plotting code for backwardpass in jax-ish, functional implementation.
did not help to definitifely find the issue. sometimes S explodes, causing
crazy unstable forward pass with basically random bang-bang input, messing up
all subsequent runs.



2024-02-16


meeting notes. talked at length about "grand plan".

suggestions for next time (in addition to what I planned to do anyways):

- think about how to compare continuous time DDP to alternatives
  (e.g. discretised DDP with Euler/RK4, or adaptive collocation+NLP)
  and see what it can do better, and what worse

- think about active learning problem/formulation.
  do we do uncertainty sampling in V or V_x? Bhavya has some points in
  favour of V, mostly that the problem is closer to the standard
  formulation, V is continuous, and there is an easy rule which V is the
  best among different local solutions (the lowest). for V_x or even u
  it is a bit hairy in contrast.

- make some sort of collection/visualisation of related works.



2024-02-19

trying to investigate the effects of evaluating d/dt solution(t) instead of
RHS(solution(t)). For the first, very easy (pretty much in linear domain)
example we already see ripples in the forward pass solution that do not appear in the initial guess (made with LQR value function). Possibly the problems I'm seeing are due to this.

turns out, there are ALWAYS ripples at least in the omega state which, while
not hugely messing up the state info, are not very small and certainly render
the derivative useless.

first tried to store the costate in the forwardpass solution using the fn argument
of SaveAt, however that does not make it to the interpolation, so is useless.
https://github.com/patrick-kidger/diffrax/issues/301

so to do it properly we kind of need to evaluate the interpolated previous backwardpass
during the current backwardpass to know the direction of the previous forward solution.

but here is the kicker: to do that, we need to know at what point the taylor
expansion (from the previous backward solution) was done. To know this we have
to know the forward solution even before that!!!

so, will this just fundamentally not work unless we are willing to evaluate a
growing number of ODE solutions summed together? This is kind of the same
issue as  line search: To do it properly we have to have an iterate which is a
weighted sum of all past iterates. This is trivial in the finite dimensional
case. But in our case  the adaptive discretization messes everything up. Is
this really a dead end? Would be very sad.

--> this "kicker" is actually not true. we only need for a particular backward
pass the forward solution immediately before that, and the forward/backward
solutions that produced it.

To make it nicer i reorderd forward and backward pass, making current
iteration again only depend on last iteration. was a whole day of refactoring.
Now it works (apparently) very similarly to before but we eliminated one
possible cause of uncontrolled numerical error buildup. Need to look more
closely at forward/backward passes still to see where it fails.




2024-02-20


maaan I am always second guessing myself. yesterday morning I already feared
that I am pursuing a complete "dead end" with no proper solution, which turned
out to be false, but now I am still fearing a dead end with no real interesting
research, only incremental improvements. Seeing how tricky it is to get the
trajectory optimiser working, just like the last time I did this kind of stuff,
I kind of wish I went with either the PINN style method, or the "brute force"
backward reachability of the PMP/characteristic equations...  what should I do?


in other news, lots of bug hunting today. more in commit message 178f9265. also
am compiling an overview of related publications right now :)


2024-02-21

meeting w/ Bhavya, Lenart, Miguel. go over list of similar works, seems fine
for them.

talk about self doubting. got reassuring reply that the goal is not to make
something great, but to make something and learn a lot in doing so :) also that
the active learning part will work quite certainly.

meeting also with Miguel at his lab to get suggestions about getting TO to
work. according to him: priority 1 = regularisation and/or line search, also
making sure that the derivation behind it is correct, probably smarter to go
with DOC style rather than "guided characteristics" style. also constraints ->
soft constraints? would remove discontinuous RHS in backward pass.



2024-02-23

spent all day yesterday tinkering with pure backward integration again, but
this time including Vxx with the formula we derived a couple times now (the one
from "third try" in idea dump specifically). It just seems too good to let go
of it! but the fundamental problem is the instability which is as fast as the
(stabilised) fastest dynamics. Maybe we can combine backward integration and
trajectory optimisation somehow?

made many notes and a pro/con list in idea dump to think more clearly about
this. essentially, the tradeoff is this: TO is the more "standard" active
learning oracle, that gives us a result at a precisely specified point, but is
expensive and also not very reliable. PMP backward shooting gives us data at
approximately the x0 we asked for, where the distance between the two probably
grows exponentially with the time horizon, according to the fastest close loop
eigenvalue.

I can think of a couple ways to do this:

- two separate iterations: initially some TO until "slow manifold" is
  sufficiently known, after that backward shooting to quickly find fast dynamics

- active learning formulation with two oracles, and some condition to choose between them

- just always do both, knowing both can "fail" in some cases.

- another combined thing. if backward shooting comes *kind of* close to the
  desired state do a forward simulation with the local quadratic value from
  backwardpass. from the new terminal state do backward shooting again. maybe do
  this a couple times?


but, all in all, the conclusion is that we still need at least a somewhat
working trajectory optimiser. two things are severely lacking:

- new, DOC derivation backwardpass (with lambda not delta lambda) is not working properly yet

- no linesearch/regularisation yet, and also questions about how to implement
  them without major refactoring. basically both need access to iteration k-2,
  or we need to modify the scan loop from the outside somehow. shouldn't it rather
  be a while loop? we don't really need the in between iterations as output. but still the
  basic implementation questions would probably be the same.



was just thinking about all the things that are left to do and it does seem
kind of overwhelming... so here is a high-level list of goals.

base goals:
- get DDP working (correctness, line search/regularization)
- try second-order Sobolev NN
- get basic active learning working w/ Sobolev NN ensemble
  (without special regard for local solution conflicts)

fancy goals:
- think properly about active learning setup with regards to sobolev learning.
- do "mixed" active learning - with characteristics and TO oracles and some decision
- implement explicit convex optimisation for u* for general case.
- address "pruning" of local solutions with NN cost function




found other possible bug - when one input is constrained and the other comes
close to the constraint, sometimes it "chatters" randomly between where it
(apparently) should be and between being at the constraint. is our u_star
wrong? damn.

possible remedy: replace the argmin (selecting from candidate solutions in
u_star_2d) by some softmax smoothed thing (just a tiny bit) and see if it
improves. bit of a hack though.

or, investigate more thoroughly and make this not happen at all! this should
not be happening, u* (i think) is piecewise linear....



2024-02-26

investigating that u* numerical jitteryness. implemented alternative smooth
version, which  finds a convex combination of all candidate solutions that
closely approximates the selection of the best candidate solution, but is
smooth, using softmax and logsumexp. does not look very much better on the
plot,  and can output infeasible solutions. would be nicer if it was always
feasible... but a convex combination of several solutions which may be
infeasible can just become infeasible not much to do about that.


when plotting the gradients (jvp in direction of (x, lam) perturbation) they
look even worse for the "smooth" version... this is probably not it.

even when plotting the values of H at the different candidate solutions, we
get some numerical jitter. and of course when the different ones are rather
close they only differ by small amounts -- quadratic in the difference between
them. these two effects combine to make the u* function "jitter" between
selecting different candidate solutions.

I have a feeling this is fundamental to any active set type method that
calculates candidate solutions, and then selects the best one, because close
to active set changes the differences in cost function are always going to be
tiny. If the comparison between different objective values is accurate up to
machine eps, then the potential inaccuracy in the solution will be sqrt(eps)!
the fundamental mistake is this: comparing two large-ish floats which are very
close together.

tried a second step where we re-center the taylor expansion around u* to
hopefully  evaluate the different Hs to greater accuracy (bc. the interesting
ones are right above 0, so better float accuracy.). same type of chattering
still happens, BUT only over a 10x smaller time interval, so definitely
progress!

the individuual solution candidates do have some numerical noise which is
probably why we're seeing the chattering. however, their individual gradients
look absolutely perfect. therefore the only source of the problems must be the
chattering which also chatters between different gradients.

"usual" explicit QP solvers solve this by explicitly storing the parameter
regions over which a linear solution map is valid. Maybe we can improve it by
pre-storing everything we can (i.e. everything except the QP parameter p =
H_u(x, u_taylorexpansion, lambda)).


what also seems surprising to me is that the function u* does not even seem to
be piecewise linear in lambda?? when only perturbing lambda not x??

maybe I do need to make the general active set solver first..?





okay, am back on debugging the main ddp stuff. it now doesn't fail obviously
due to the chattering anymore, and the chattering is not visible anymore even
with very fine interpolation. HOWEVER. it still routinely goes wrong. the
moment it does we seem to have a very negative eigenvalue of S(t), and it
starts being negative close to a moment where the input constraint active set
switches. still a bit sus...



2024-02-27

wrote some stuff about mpqp. in case i need it i now know how to prune the
active sets that are never optimal (by solving an LP feasibility problem for
each one).


back at debugging positive definiteness. scoured through all ct-ddp papers i
could find, and it doesn't look like any of them claim that the hessian stays
positive definite.

can we use sqrtm(S) instead as an ODE state? but probably ugly things would
happen if the ODE wants to make it non-pd regardless...

apparently the ricatti eq. itself does result in pd solutions if inputs are
pd.

managed to rewrite both the ricatti eq. and our Sdot in a way that makes it
obvious they are one and the same (write the quadratic in terms of S as  [1;
S]' [some matrix] [1; S]). so in theory I think the solutions should stay
positive definite as long as the input cost hessian l_uu and thus H_uu are.


also, tried regularisation. no scheduling of any sort, just constant penalty
of distance to previous trajectory, weighted with Q matrix. seems to do what
it should. choosing the scaling of the penalty term dynamically will be the
far greater challenge.



trying to understand what precisely doesn't work about the new DOC RHS
function. changing the old one step by step to hopefully either arrive at the
(correct) new version, or to find the change that breaks it.

got pretty far actually, without changing the look of the solutions at all.
everything is now in terms of linearisations at the forward trajectory. but
still it might precisely correspond to the neater DOC derivation.

still getting similar instability issues, it almost looks like again it is
caused by chattering at the constraint switching time... there are some papers
about optimisation constrained ODE solving in the cs/numerics folder which
might after all be not a bad idea. basically they reframe the ODE with
optimisation problem in its RHS as a DAE with the corresponding KKT system.
But the KKT system has to be changed on active set changes and detecting this
event is apparently not so trivial. still, maybe we can coax diffrax into
doing this???

after looking at all forward-backward plots it might also be that accepting
the full step messes us up. a couple iterations before disaster there are
already some oscillations which look menacing...



2024-02-28


put the explicit convex optimisation notes in latex. found a paper presenting
the same concept, checking if some active set is ever optimal by solving an
LP. they also have a nice method to do extensive enumeration with a
tree-pruning strategy allowing them to skip over many active sets, knowing in
advance they are not optimal. if we scale to larger examples this might be
pretty smart. but for the moment let's ignore this.


found a mistake in DOC derivation! fixed it, in dump section 4.6.2,  "Backward
pass with actual λ instead of δλ". tried running it, got interesting results.
no wild instability anymore, instead the trajectories all seem to be very
chilled out, without the aggressive down-braking maneuver that showed up
before. very weird.



thinking about a possible change in perspective. up until now the general goal
was this: find the optimal control law, using TO and/or characteristics, as
efficiently as possible.

maybe efficiency first is kind of dumb? maybe it would be nicer to start with
the "brute force" backward PMP reachability type methods first? which also
gives us a grasp on global optimality if we always analyse value level sets.
this would be essentially an adaptation of standard dynamic programming /
bellman recursion to continuous time, which certainly has its kinks but the
general structure and also probably complexity should be about the same as
discrete time DP.

and then from there, see how much we can optimise and "cheat" our way around
the limitations for physical systems which, far from the origin, have a clear
timescale separation going on, but while still retaining the benefits of the
full optimal control law close to the origin.

is this dumb? tbh if we do this and then end up speeding up the exploration
along slow  manifolds using TO we will probably arrive at exactly the same
thing we're building right now.


played around with backward integration and time-value reparameterisation
again. got not much further than a couple of plots though...



2024-02-29


cooked up some thoughts yesterday evening. if we treat trajectory optimisation
not as the omnipotent tool (which it is probably not) but as a heuristic to speed up the brute force backward PMP reachability computations, then we are probably better off. let me explain.

as discussed the two methods (TO and backwards PMP reachability) have
complementary benefits. Long turnpike trajectories in multiscale systems
completely mess up the backwards PMP approach due to frequent resampling, and
a data generation which is more dense than what would be needed for NN
fitting, which is needed to even find the slow, turnpike-like trajectories.

TO can do that very nicely however! I am proposing a two-step scheme like this.

1. collect, with TO, a database of many trajectories, from x0 where the fast
states are either zero or "already stabilised". point being, we are interested
in long trajectories mainly.

2. using backward PMP, "largest underestimator" type NN, and growing the
dataset as increasing value level sets, approximate the full backward
reachability computation. the advantage is that now that we have a good
approximation of the value function on and close to the "slow manifold" we can
relatively easily find optimal trajectories to other, close states with pure
PMP backward shooting, without (huge) sensitivity problems (i.e. we propose
some x0, simulate forward with approximate optimal controller, find optimal
trajectory in backwards time, and probably we land close-ish to the state we
started). Viewed from another lens, we can now circumvent the problem of
having to generate "too many" trajectories for the purpose of finding the
turnpike, because we already have it. instead we can do a relatively sparse
sampling of those trajectories, until the BNN has enough data to be confident
but not (much) more. See sketch.


flashy title: using optimal trajectories to speed up backwards PMP
reachability level set optimal control computation in multiscale dynamical
systems. hehe


found the mistakes!!! what I called H_x in the code was not actually H_x. now
the two backwardpass rhs's should be the same. though I almost feel like it is
more brittle and fails sooner now... at least we now have a theoretical basis
for doing line search or regularisation.

am making a new plotting function to see if the lambdas and S matrices
actually look like gradients and hessians of v, at least in the direction of
the individual trajectory. looks quite alright. tbh if this wasn't correct
we'd probably have bigger problems that showed up sooner




2024-03-01

did some experiments where I tried to find the root cause of the negative S
eigenvalues. did the following
- define the backwardpass also in terms of the cholesky factor of S
- plot det(S) after running

unfortunately I never got to the failure case because now it fails much
sooner, when S grows very large on a long-ish time interval where both inputs
are at the upper constraint.

though, i could always test the same thing in the current
backward_with_hessian  example in flatquad_landing_experiment.py -- there we
do see negative hessians regularly. next week \_o_/

am really debating whether it is worth it to continue working on the TO stuff.
on one hand, it would be very cool to get it to run, and as talked about just
above, would undoubtedly be able to accelerate the "pure" PMP backward
reachability approach. On the other hand, I've now invested 2 months into
this! well, looked back into log and commits, and I've only really started
this one month ago. january was spent entirely thinking about backwards PMP
type things. so maybe the sunk cost fallacy is not huge here after all :)

but more practically, it would be cool to start working on the learning part.
should I just go ahead and do this with backward PMP regardless? there are
still many nontrivial questions to be answered there, even if we completely
forget DDP stuff:

- which new points to propose?
  2 main strategies. a) sample statespace points and select good ones or b)
  start in the "interesting set" (BNN uncertain within value sublevel set) and
  explore it with some MCMC type thing

- how to do bayesian NN?
  ensemble, stein, others? probably ensemble should be enough for a start

- 2nd order Sobolev
  this I think is actually what I think could be the nicest of all things. Calculating
  Vxx amounts to having feedback gains, enabling us to treat the nonliear system with
  linear systems tools, instead of just having control inputs at some points. given that
  most usual systems *are* pretty close to their linearisations over large-ish regions
  we should expect to make a decent improvement on brute force backward reachability
  by doing this. maybe, who knows \o/

- local/global?
  by now I think I have a decent grasp on this. If we learn V(x) as successive value sublevel sets this should be pretty natural.
  the question is then how quickly can we "fill" this sublevel set with data, especially in the direction
  of slow manifold/turnpike trajectories. but I think it should be manageable, we just have to live with
  dataset size O(timescale ratio) multiplied with whatever complexity we assume in
  terms of state space dimensionality.

- better strategy for u*?
  the current one works after the hack to minimise numerical chattering. certainly
  cleaner would be active set enumeration + selection of the solution with smallest KKT residual.
  from there there are millions of mpqp tricks, pruning of never-optimal active sets,
  decision tree structure, actual active set method running online but with precomputed
  KKT systems of some sort.
  even fancier, the suggestion of patrick kidger, where we extend diffrax with
  more flexible event handling, detect an active set change, and somehow give the
  active set to the RHS in args. as a boolean vector? as a finished linear map from
  parameter to u*? Lots to be done there still. but probably best to put this on the
  back burner as the solutions here are kind of the most obvious.



2024-03-04

am investigating non-positive-definite vxx with pure backward integration. added
the state logdet(S) to the ODE, with RHS very trivially evaluated with jax:

\dot logdet(S) = d/dt logdet(S + s \dot S)

as expected we get many small steps when det(S) is close to 0, i.e. logdet
approaches -infty. which it still does! so unfortunately it seems like the
determinant turning negative is not just a fluke but actually what happens for
the solution of these ODEs.

okay, new info. if we compare the ricatti equation to standard form, we see
that the Q matrix (constant term in quadratic rhs) is very non-pd when both
upper input constraints are active. y dis?

maybe this is just a thing that happens (for general systems and "unsuitably")
defined costs... Maybe we just have to live with the fact that the value
hessian is sometimes not positive definite? I have double checked that we have
the correct derivatives forming the Q and R matrix for the ricatti eq and
found no mistakes...

does this correspond intuitively to the fact that the feedback can be locally
unstable in some directions at some points?

alright, thought about it some more. am now pretty sure that this is a thing
that happens normally. drew a picture to convince myself -- a great variety of
2D, single-integrator navigation-type examples will do, as long as there is
some sort of nonconvexity in the cost funciton. so, forget this, negative
hessian eigenvalues are okay too!


found a couple additional papers applying pretty much my ideas (backward
characteristics following, level set, remeshing) to 2D and 3D single
integrator navigation examples. pretty cool for inspiration. but higher state space
dimension will certainly bring its challenges.

atm I am doubting if we will get sobolev to learn an accurate hessian. It (the
function x -> Vxx(x)) will definitely be nondifferentiable at some points,
though continuous. this will be a bit of a challenge for the NN (maybe). if we
then do the resampling step of forward simulating with approximate V, then
backward simulating with the PMP, we carry with us this wrong hessian for all
time. ways to remedy this could include:

 - accepting the inaccuracy and only using the hessian as a "small hint" for how
   the value function should roughly look like

 - writing some analysis to show that the error becomes less significant with
   longer time horizon (akin to continuous time, finite horizon LQR for LTI
   system -- quite soon the influence of P(T) vanishes)

 - making sure that the sobolev NN *does* actually approximate the hessian precisely!
   maybe this works better if we have separate NNs, one for V(x) and one for lambda,
   with just one additional order of sobolevisation? but not shure why this is just
   a wild guess based on intuition and would make other things harder too.




2024-03-05


set the goal of making a very basic mockup of the backward shooting, NN
ensemble fitting, forward simulation loop. recently I've been thinking about
this and gained two new pieces of intuition:

first, learning the value function in level sets should make global optimality
much easier, because whenever two solutions "meet", they are of equal value,
and thus can both just be stopped. (intuitively). in practice it will probably
also have something to do with the extrapolation outside of the current level
set kind of becoming an interpolation before the level set starts a self
intersection. t

second, the *only* difficulty with instability is finding long trajectories
along the "slow manifold". this is not really new, but this also means we
probably don't have to do much resampling in the fast dynamics part. therefore
everything *but* the slow manifold trajectories are kind of almost free. (free
lunch hehe). we can integrate them further than we need too: e.g. stop only
once v(x) >= 10 * v_k, where v_k is the current value level set. for learning
we can just ignore the data above v_k. otherwise we'd have to do many many
short simulations at each value level increase, and then some long ones too
for the slow manifold. (just a small implementation detail tbh)


got scared of the huge task ahead of me and procrastinated.

instead got confused whether what we call vxx in the backward ODE is actually
the hessian or the half hessian or twice the hessian. am pretty sure now that
ode_state['vxx'] is the half hessian. we initialise it with P_lqr which is
definitely the half hessian of the LQR value function x.T P_lqr x. This causes
the vxx eigenvalues to stay about constant in the linear region, indicating
that the two are the same.

HOWEVER, I did the hessian sanity check plot again (evaluating the taylor
expansion along the trajectory) with the new dict solution format from pure
backward integration. am not sure if it looks better with 'vxx' or 2 * 'vxx'.
and all the other formulas (d/dt vxx mainly) seem to be consistent with
whatever is implemented now.  definitely should look at this again.

also put the "purely backward" stuff in its own class again.

spent all day making this kind of plots and sanity checks. am not much wiser,
also no prototype of backward shooting with NN fitting loop yet. todo next
time \o/



2024-03-07


ran some experiments of generating many, many trajectories, then plotting the
ones with lowest v(0) (start of physical time = end of integration time).
looks cool, there are trajectories which come from a bit further away, however
it is obvious that purely random sampling will not lead much further. that
much I knew beforehand.

continued a tiny bit on the writeup.

took the nn implementation out of the dusty drawer and made it run again.
didn't find very nice hyperparams just jet, but looking promising. maybe it has
trouble with data being on a "small" scale? maybe we need to build in some data
normalisation...

Quickly before heading home, figured out the adjustments for gradient under
linear coordinate transformation. is pretty simple. implement tomorrow.

also tomorrow: finish/clean up that writeup...


2024-03-08

made a nice data normalisation class, accounting for how the gradient gets
squashed under linear coordinate change. looks much better with normalised
data!

spent some time tuning nn hyperparams. low final learning rate substantially
improves stuff! plotting the NN at states visited by a trajectory gives the
expected result: decent fit in region where data was used (this case, value
sublevels set 100), flailing around otherwise.


went on a small debugging spree to do with LQR terminal value or half of it.
it seems that the optimal controller (backward PMP) gives costates similar to
the value function 1/2 x.T P_lqr x. However the terminal value function was
assumed  to be x.T @ P_lqr @ x. I really suspect we are doing something wrong
here. for now  I just replaced everything by the value function that "looks"
more correct, but  sometime i should def. get to the bottom of this.

plugged the nn costate into a closed loop controller. took me all afternoon to
realise that here we need to undo the normalisation...

still it is not trivial to find hyperparams that lead to a good fit. some
gradients are still much smaller than unity. maybe we can tell the nn loss
function about that and have a penalisation relative to the typical scale on
which the individual gradient entries fall?

by now I think the focus/goal should be specifically to improve the backward
PMP stuff with vxx propagation and sobolev learning. The regular PMP backward
reachability/level set method is interesting but its limits are kind of
obvious. more interesting is how we can take inspiration from it and make
something better. atm I do think that vxx/sobolev shows great potential, by
basically fully using linear systems tools to help in learning the nonlinear
solution. so if we happen to be lucky and the linearised optimal controller
(around some trajectory) is accurate over a large-ish region, at least in some
directions, we profit by making faster progress (albeit with more expensive
simulations.)


another small experiment: if we train only with data close to equilibrium (v <
0.1), then the hessian of the nn value function is almost precisely already the
LQR hessian (relative norm difference 0.032)!! WITHOUT ever telling the NN
about the hessian! (although v_x is probably practically linear so recovering
its gradient is probably not a huge miracle)

still, this is quite promising and certainly makes it look like we can use
second order sobolev learning to get an even better fit without huge amounts of
data, and more importantly, better extrapolation, which will speed up our
"filling" of the state space when the value function is similar to its
extrapolation (i.e. basically the quadratic local value function)


next tasks:
- extend this to NN ensemble (was this as easy as a well placed vmap?)
- think about practicalities: how to propose high-uncertainty points?
  how to handle level set collisions? how to decide on v_k+1? lots of this stuff
- implement second-order sobolev functionality, take the chance to use
  a nicer data format in the nn class, maybe also vmapped dict like ode solver?
- also take that chance to sanity check the vxx calculation, esp. with regards
  to the now fixed mistake in choosing the LQR value function...



2024-03-12

second order sobolev is implemented and works (not sure why no log entry
before...)

performed a parameter sweep for the sobolev weights from (1, 100, 1e-2) to (1,
100, 1e+4). In this particular case (data from initial uniform backward
shooting, max value level 1.0 -- exact code in commit 5e43e169), we have a
sweet spot about at 100 or maybe between 100 and 1000, where the sobolev loss
noticeably improves vxx accuracy but doesn't mess up the v and vx yet. I'd say
100 in this case is the point where it completely doesn't worsen vx and vxx.
interestingly, and as I'd hoped for, there is a long slow decrease in vx loss
as we increase the vxx weight from about 1 to 100. only slight though,
probably a factor of 2 or 3 on avg.

plot is in losses.png file, root dir but not in git.

this is essentially the training loss and the test loss at 0, which is not a
very strong testament to the validity of this modelling approach. we should do
proper test loss evaluation and hyperparam selection via cross validation. but
this serves as an initial sanity check that it kind of does what we expect and
gives us parameters that are better than just smelling some numbers and hoping
for the best.

update, made the actual test loss plot. train/test splitting function in
nn_utils. test loss looks much the same as train loss except maybe a tiny bit
higher.

tried increasing value level to inf (= including all points) to fit more
interesting function. seeing problems where ONE SINGLE trajectory has vxx
values that blow up to like 1e30 and then become NaN. investigating this atm.

am with ipdb in the first rhs evaluation that has ||vxx_dot|| > 1e5. not sure
how to debug, all the numbers look normal-ish...? should I make a second
f_extended that outputs all intermediate values, to plot them?

yes, did that, using the previously unused args keyword required by diffrax.
every variable looks like a reasonable function of time, just d/dt vxx as given
by the function starts containing crazy entries. whyyy?


seems the vxx components are unstable for already a few timesteps before.


at this point i have the feeling that somehow this is just a thing that
happens, that the hessian can become wildly large... it happens before a quite
long segment where both upper constraints are active too. so high sensitivity
to state space position and thus high feedback gains might be kind of
expected here...

next attempt: switch to 64bit float precision. it seems we do not get nans
anymore! however the data are completely different, as probably the PRNG is
essentially progressing twice as fast.

next: find bad solution in 32 bit, serialise whole ys dict with flax msgpack,
switch to 64 bit, read it again. try to reproduce the solution with 64 bit and
see what happens.

IT WORKS! more precisely: the spot where previously the entries and
eigenvalues of vxx shot towards infinity is now quite well behaved. instead of
going up wildly the vxx entries actually decrease rapidly after the
constraint  stops being active (reasoning in backward time here). during the
constraint it goes up some time which also makes sense. decreasing dtmin by a
factor of 10 (now) makes the transition even nicer.

need to see afterwards if in 32 bit we can also just decrease dtmin and have
everything work... UPDATE it does. that was a bit of a detour \o/

however, the problem is not entirely gone. if we continue the trajectory for
some time, there is a long segment where both constraints are active, one
upper one lower. the trajectory corresponds to slowing down from a state with
incredibly high omega (and phi, but bc the angle wraps it is the same as (-pi,
pi). TODO transform to (sin, cos) for learning to reflect that topology.). at
some point the constraint stops being active and then the problem appears
again.

that state however is so outlandish we might as well forget it. practical
solutions possibly are:
 - stop integration after we exit some large superset of the relevant states
 - stop integration after some number of steps (this happens anyway...)
 - stop integration once vxx, its norm or eigenvalues exceed some limit?
 - in addition, throw away any points where any part of the state is nan
   or even just above some sanity limit? or also remove the preceding couple
   moments from the trajectory?

possible too: go to a handful of steps before the issues, perturp extended
state slightly, re-run.


after smoothing out those issues, tried to fit the sobolev NN on the whole
dataset. results are much less exciting than for 1-sublevel set of v. loss
goes down at the start but barely low enough to be of any use. training loss
that is. possible reasons:
 - large/small numbers despite normalisation messing it up
 - low-loss solution "too far" to reach in amount of iters
 - conflicting local sols (although intuitively i dont think so)
 - wrong vxx values making impossible for the loss to go near 0?
 - vxx which is not wrong per se but a "too complicated" function (it is
   made by integrating an only piecewise continuous rhs after all....)

made epochs 10x longer and final lr 10x smaller, also set value level to only
50. after that it looks not too bad again (but takes 2min for 10000 pts). vxx
testloss between 1e-4 and 1e-4, vx between 1e-4 and 1e-5, v between 1e-5 and
1e-6. i think this is alright, although it does suggest we might increase the
vxx weight. (is it a good tuning rule to make all losses "equally low"?)


next steps, mostly carried from yesterday:
- find out why the nn is shit for nontrivial value function. find some tuning that
  sorta works.
- NN ensemble (or something similar w/ multiple initialisation, SGLD, whatever)
- think about practicalities: how to propose high-uncertainty points?
  how to handle level set collisions? how to decide on v_k+1? lots of this stuff
- make first (hopefully, barely) working draft of main iteration! even if shit.


2024-03-13

still trying to find good nn tuning. currently doing grid search over
relative weights for vx and vxx loss.

probably the very basic reason that the value nn works shit is that we are
asking too much. if we set the value level relatively high, but only train
with data from initial backward shooting from uniform terminal states,
there will be large empty regions, where we cannot hope for any sort of
good fit. if v_k is smaller indeed it works much better: vxx test loss goes
down almost two orders of magnitude by switching on vxx training, and vx
loss goes down as well! just like i was hoping for. though, this is still
probably pretty close to a quadratic value function and not an indicator of
real world performance.

some new thoughts. if we want to have value sublevel sets we should really
make sure that the value function does not go down outside of the usable
range. maybe this happens automatically? if not, we could try adding some
sort of "prior" loss that pushes the value function up slightly (in a much
bigger region than where we have data so the influence there should be
negligible). maybe also penalise norm of the hessian similarly, to
encourage smoothness?

also, how do we evaluate whether we "know" the value function in a certain
sublevel set? or even better, directly what is the largest value sublevel
set in which the information is accurate?

two ways: posterior uncertainty of some ensemble and cross validation. if
doing the first we need to calibrate (via cross validation probably). is
this just a detour?

just for fun calculated "overparameterisation" ratio, n_params / n_data.
for all practical intents & purposes each entry of vxx is just another
label which we have to fit, although with particular "meaning" and with a
modified (differentiated) NN but with the same params. Therefore we
calculate just naively the total number of floats in the params and data
pytree.

I expected to be in the underparameterised regime but not that low: it is
0.005! half a percent! (for that particular dataset and (32, 32, 32) nn).
thus it should also be expected that we cannot reach zero, or very low,
training let alone test error.

trying the same thing but with (1024, 1024, 1024) shaped NN. This gives us
overparameterisation at a ratio of 4.312. took 5 min to train! aaaand it
completely failed. looks like lr was initially way too large, then just
right for a tiny moment, then too low. not going to fix that now, fat NNs
like this are impractical anyway (unless we put it on euler or google
TPU.... should do that sometime anyway)

other than that, current settings are pretty decent. not what i kind of
hoped for (much lower vx test loss when including vxx), but still more
than 10x improvement in vxx test loss compared to not including it at all,
and like 2-3x better vx loss. takes 1-2 s to train on laptop.


meeting. main input: describe the approach at a very high level in clear
and known terms. level set/dynamic programming -> level set "expansion" as
active learning problem -> oracle = backward PMP or TO. without any
implementation issues & details, these come later.

functional priors: this idea exists and bhavya did some work on it! i

BNN: find last layer weight distribution using BLR with previous layers
fixed. simple but apparently this works!




2024-03-14

big refactoring spree. cleaned up current testbed, put backward solver in
place of the old one in pontryagin_utils, etc. 


tried nn ensemble with jax.vmap, only altering training key (responsible for
batch selection and stochastic sobolev approximation). looks alright actually!
ofcourse takes nx more time. other, more sgld inspired approaches might be
nicer, then we only repeat the "tail" of the training. 

for N=8, we have nice uncertainty bands that roughly correspond to the actual
trajectories in the training set. as reported by many others we have
overconfidence outside of the training region. if this is problematic we can
switch to repulsive ensembles..? but probably it will work just fine without
repulsiveness. 

still this only works for low value level sets and breaks down completely for
v_k > 100 or so. still don't know why. list of possible reasons in log entry
2024-03-12 towards the end. 



2024-03-15

taking a closer look at the output of data normalisation to see if maybe vx
and vxx (which we can't just freely normalise) have very unsuitable magnitudes. 

but it looks pretty reasonable unfortunately. v has obviously zero mean and
unit variance, and so does each x component. the vx entries have means mostly
below .1 in magnitude, and variances below 1. vxx has most means very close to
0, and some ranging up to +/- 5. stds mostly between 1e-3 and 1, about uniform
looking on a log scale, with one ranging up to almost 10.

as it stands now none of these numbers should be a reason for concern, in fact
they look nicer than i expected. though i can definitely imagine situations
where it might turn out worse.

the intuition i am currently subscribing most to is this: during training for
the more "difficult" datasets we see generally the vxx loss being the highest
during training, especially in the long tail section. combined with the fact
that the vxx weight is mostly on the same order of magnitude as the other
ones, i believe that the total gradient signal is dominated by the vxx loss,
slowing down progress on vx. current experiment: try lower vxx weights. maybe
we can split the training into phases? first train with vxx weight 0 until we
have a decent fit, and then enable it again. or do some trick like clipping
the influence of the vxx gradient? but this is probably a bit of a hassle and
i would really rather specify a loss function and use a standard optimiser
than tweak the optimiser when really we are kind of encoding a different loss
function by doing so. 

i also have the feeling that the vxx loss, and to a degree the vx loss, may be
more "ill conditioned" as a function of nn parameters. not sure why though. 

anyway, with a lower vxx weight it is actually not that bad! we get 1e-2 ish
vxx test loss on the full dataset (4 second uniform backward shooting, 200
trajectories.). the trajectory vs nn plot looks alright, not very thrilling,
certainly approximation could be made better still, but the overall shape of
the vx functions is well captured. 

closed loop testing of the resulting controller is also pretty nice! for
initial states sampled from N(0, 5I) we see probably about half of them being
stabilised. although, I feel like once the fast system (phi, omega) is
stabilised they kind of move inwards "too slow"? should compare w/ lqr again
to be sure, maybe not. 

but nevertheless, I think it's impresseive that even though the trajectories
generated are from a *very* specific weird distribution (fast system
immediately excited in backward time, so no progress to speak of on slow
system), the stabilisation works for a relatively wide range of initial
states! this is promising, as it is exactly this type of extrapolation we are
planning to rely heavily on. 




did an experiment to find out how many epochs we actually need. learning rate
from .01 to .0001 exponential decay. epochs 2 ** np.arange(14), largest 8192. 
while longer training does usually help, it looks like the last two at least
are a case of overfitting, where test loss is higher again. redoing it up to
2**12 epochs to look at that more clearly. 


still the fits are pretty shitty once we go above values of about 1000, or
where the input constraint switching starts to become more regular...


made a small utility function to plot the cdfs of the different normalised
variables. maybe we find the reason of our shitty fits? although, the
distributions seem very reasonable, even for the thin high value band cases
that fail completely with current settings...








aaaaaaaaalso, been thinking about state space topology. if we want to make
global solutions make sense, then the state space should be uniquely
paramerised (does this have a precise meaning?), and right now we have the
angle which is mod 2pi, so several state vectors for the same physical states.


a transformation Phi -> (sin Phi, cos Phi) solves this easily. but introduces
new questions.  

if we transform the whole system, before simulation, there are some more concerns.

  the state could leave the manifold ||sin Phi, cos Phi|| = 1. Baumgarte to
  the rescue!

  more importantly, does PMP change significantly when we go to manifold state
  spaces? skimmed some papers on this and it seems very cumbersome. i am ready
  to learn the basics of differential geometry but not about to do a math
  major! but this is probably nevertheless the "most correct" solution...
  maybe i should ask some of the control guys at ifa if they happen to know it
  even a bit? 



if we transform this after simulation, just for nn training,
then how do we transform the costate, and vxx?

  probably this is some not too crazy derivative trickery. just that the
  derivative is not defined in all diretions anymore. i mean it is still the same 
  basic change -- we now have a function defined on a manifold, and so are its gradients
  so this still needs to be treated in the correct way if we do it like that. 


2024-03-18

took another look at the deteriorating training with higher value level. found
suspicious vxx entries (like 1e17, surely this is an instance of the random
instability rather than the actual solution). 

took vxx loss out completely. now if we specify only 2 sobolev weights all vxx
stuff is skipped entirely. at first glance it looks like the fit is indeed
(practically and qualitatively) just as good as the one with vxx -- still we
have a much larger RoA in closed loop than the training dataset which is very
nice. all while saving lots of computation. was calculating vxx misguided from
the start?? but this still for v_k = 100. 

increaing it to 3000 where previously it failed. got nice training loss curves
but unbearably high test loss. whyyy? visually the fit looks alright though,
with vx curves quite close to training trajectory vx but oscillating around it
(certainly with very bad vxx approx)

not able to get better results with training on v and vx only after doing some
parameter tuning. also not able to fix vxx issues. whaaat do


plotting the offending trajectories it seems that they are all ones where
angular rate is first in one direction, then changes, before finally
stabilising at the equilibrium. does this somehow connect intuitively to the
toy example above? 



-> did some more thinking about vxx growing unbearably large. made a long
writeup here, but ended up converting it to latex. is now in idea dump.
conclusion: vxx terms can definitely go unbounded, and specifically when an
eigenvalue goes to -infinity it corresponds to a starting self-intersection of
level sets, or equivalently a focal point of characteristic curves. 



... did some more tuning, switching on and off the vxx loss,
over/underparameterisation etc. got pretty much nowhere, same things work as before.

interestingly, when trying to fit a thin value band (example from 250 to 300),
the train/test losses look really quite bad but the trajectory vs nn plot
seems to indicate that the fit is pretty nice within the value band, and even
extrapolates a bit. 



2024-03-19

experimenting with NN ensemble. specifically, the sort where we take multiple
parameter sets encountered during training, and basically repeat the "tail" of
the training loop N times. Like SGLD but wihthout proper noise injection, only
relying on SGD noise. 

This seems to be much worse than just vmapping the entire training loop
though. We get very overconfident predictions outside the region of training
data, barely larger uncertainty than within. OTOH, with vmap they are still
overconfident but seem kind of reasonable and *do* grow once we leave the
training data region. also, vmapping is just easier, and actually not even
that much slower than training a single one! maybe a single one doesn't max
out the vectorisation capabilities (of my shitty local cpu...)

I will go with the vmap approach for now. it is simpler, slightly brute force
for sure, but I am fine with that (especially if we switch to Euler/TPU or
whatever.). Also it is much more obvious how we might include "functional
priors" there: just generate a set of random points and labels at the start of
training -- vmapping with individual PRNG key will also randomise this prior,
thus we should be able to get even better model diversity in this way. 



ALSO recently several things were on my mind where we are not using the
"proper" implementation and I always feel the itch to change it to a better one. 

- u* optimisation: this would surely be better addressed by detecting active
set switching events, stepping exactly to those, then changing the RHS. kind
of like that paper about optimisation-constrained ODEs as DAEs. ATM the
adaptive solver + explicit mpqp(-ish thing) works, but generates much more
data than necessary. maybe just subsample the data before giving it to NN?
although this should be qualitatively the same as using the whole data set but
fewer epochs... also same as that old question about resampling trajectories
with the interpolation to be evenly sampled...

- fundamentals about the state space being a manifold, see bottom of
2024-03-15

tell this lenart&bhavya to get predictable but reassuring answer to not worry
too much about those? 


meeting traktandenliste:

mainly:
 - go over active learning problem formulations.

"daily business" recap:
 - realisations that Vxx can have infinitely neg eigenvalues
   -> need practical fix to avoid messing up nn training
 - NN ensemble experiments
 - stuff is quite slow. beg for like the third time for a little euler tutorial? 

other questions:
 - administrative: when is handin date? when are those 2 weeks of vacation? 
 - any guidelines on the shape of the final report? then i can start writing down basics
 - how to stop myself from endless & mindless parameter tuning?!?

